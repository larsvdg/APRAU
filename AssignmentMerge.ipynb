{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "085441d1-8a11-4750-8261-64e83961495a",
   "metadata": {},
   "source": [
    "# Assignment - APRAU Group 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c7f284-849f-4685-9f19-6f1366807801",
   "metadata": {},
   "source": [
    "**Student - Student number**  \n",
    "Ladislav Gardian - 1240524  \n",
    "Lars van der Griend - 1240271  \n",
    "Peter Likavec - 1240523  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54889bb-92ac-493b-ae0a-d8f2e95617d4",
   "metadata": {},
   "source": [
    "ToDo:  \n",
    "- Encode target variable (use OneHotEncoder) since there is no order in the target variable and regression models treat the encoded values as different classes and not as ordered values. Therefore, also maybe use factorize instead of dummies but need to figure that out\n",
    "- further data preprocessing (dummys or factorize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c6c59aea-1d4f-4e7c-95ae-821b8a07c623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, Ridge, RidgeCV, Lasso, LassoCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import warnings\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827e4108-d75f-4a59-80b6-230762ffb99c",
   "metadata": {},
   "source": [
    "For every group, three datasets where available for creating a model to predict the Vegetaton Type. This classification problem will be solved by applying Machine Learning models. To train and test such Machine Learning models, all three datasets are loaded and merged into one big dataset which will be used in the rest of the Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "120520e0-1566-4e89-a5e6-28857379d203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from the three available datasets for Group 1\n",
    "df_1 = pd.read_csv(\"Data_Class_1.csv\")\n",
    "df_2 = pd.read_csv(\"Data_Class_2.csv\")\n",
    "df_3 = pd.read_csv(\"Data_Class_3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "694aa854-afaf-4314-bd72-ddedfad76dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the DataFrames \n",
    "df = pd.concat([df_1, df_2, df_3], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35665e83-60a7-4d71-afc5-0e1a221fa288",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc732673-c450-414e-9241-68078427c28d",
   "metadata": {},
   "source": [
    "### Exploratory Analysis\n",
    "\n",
    "To get a better understanding of the data, an exploratory analysis of the dataset will be done. This exploratory analysis can give prior insights on the data, can help understand the data structure and dimensions better, and much more. The subsections below will guide through this exploratory analysis in steps.  \n",
    "\n",
    "**Descriptive Statistics and Data Types**  \n",
    "Using built-in Python functions, the descriptive statistics of all numerical columns in the dataset can be summarized, see the Table below. The *count* for every column equals 5184, indicating that there are no missing values. Furthermore, the data type per column is shown. Excluding the target variable *Vegetation_Type*, there are two more categorical columns; *Soil_Type* and *Wilderness_Area*. The other columns all contain numerical values. Moreover, the number of unique values per column is shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a836c663-5389-4b43-88de-1ed233cac3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22e6f6f-f6cf-4448-8b50-77ad0547f16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types and missing values\n",
    "data_info = pd.DataFrame({\n",
    "    'Data Type': df.dtypes,\n",
    "    'Missing Values': df.isnull().sum(),\n",
    "    'Unique Values': df.nunique()\n",
    "})\n",
    "\n",
    "data_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67b358c-9096-4938-a445-92a7acd799ef",
   "metadata": {},
   "source": [
    "**Univariate Analysis**  \n",
    "Now the distribution of the data in individual columns (i.e., features) can be investigated. Understanding the distributions of the features ir=s relevant for later data analysis. Based on the distribution of the features, different methods should be applied. First, the distribution of the numerical features will be checked by creating histograms. The density curves are added to help understand the distributions. As can be seen, the features all have different distributions. Some Uniform, Gamma, Normal and other distributions can be recognized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef13a04-ee95-434f-aec0-d4102ad1d406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the style for the visualizations\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# List of numerical columns to plot\n",
    "numerical_columns = [\n",
    "    'Altitude', 'Slope_Orientation', 'Slope', \n",
    "    'Horizontal_Distance_To_Water', 'Vertical_Distance_To_Water', \n",
    "    'Horizontal_Distance_To_Roadways', 'Shadow_Index_9h', \n",
    "    'Shadow_Index_12h', 'Shadow_Index_15h', \n",
    "    'Horizontal_Distance_To_Fire_Points', 'Canopy_Density', \n",
    "    'Rainfall_Summer', 'Rainfall_Winter', 'Wind_Exposure_Level'\n",
    "]\n",
    "\n",
    "# Set up the subplots, adjusting number of rows and columns to fit all features\n",
    "num_plots = len(numerical_columns)\n",
    "cols = 3\n",
    "rows = num_plots // cols + (num_plots % cols > 0)\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(18, 12))\n",
    "fig.suptitle('Distribution of Numerical Variables', fontsize=16)\n",
    "\n",
    "# Plot histograms for each numerical feature\n",
    "for i, col in enumerate(numerical_columns):\n",
    "    row = i // cols\n",
    "    col_idx = i % cols\n",
    "    sns.histplot(df[col], kde=True, bins=20, ax=axes[row, col_idx])\n",
    "    axes[row, col_idx].set_title(f'{col} Distribution')\n",
    "\n",
    "# Hide any unused subplots\n",
    "for i in range(num_plots, rows * cols):\n",
    "    fig.delaxes(axes.flat[i])\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fc491f-75bb-4307-ac9e-7b51ea67bb0b",
   "metadata": {},
   "source": [
    "Now, using barcharts the distribution of the non-numerical features (excluding the target variable) can be visualized. It can be seen that for the *Soil_Type* some values occur very often, and some occur only a few times. The *Wilderness_Area* distribution is a bit skewed as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d0c822-c853-4a3b-a4cb-1fdf486a2770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot bar plots for categorical variables\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.suptitle('Distribution of Categorical Variables')\n",
    "\n",
    "sns.countplot(x='Soil_Type', data=df, ax=axes[0], palette='Set2')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].set_title('Soil Type Distribution')\n",
    "\n",
    "sns.countplot(x='Wilderness_Area', data=df, ax=axes[1], palette='Set2')\n",
    "axes[1].set_title('Wilderness Area Distribution')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c60eea3-2c7a-42a2-ac09-5382e63751dd",
   "metadata": {},
   "source": [
    "**Bivariate Analysis**  \n",
    "Now we can investigate the relationship between the individual features and the Vegetation Type. Since the problem of predicting the *Vegetation_Type* is a classification problem, to identify patterns between the numerical variables and target variable, violinplots are used. Violinplots give the distribution of the numerical value, given the target variable *Vegetation_Type*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268018a0-6827-4c72-81d8-e22c10339bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the subplots, adjusting number of rows and columns to fit all features\n",
    "num_plots = len(numerical_columns)\n",
    "cols = 3  # Number of columns in the grid\n",
    "rows = num_plots // cols + (num_plots % cols > 0)  # Calculate required rows\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(18, 12))\n",
    "fig.suptitle('Scatter Plots: Vegetation_Type vs Numerical Variables', fontsize=16)\n",
    "\n",
    "# Plot violoinplots for each numerical feature against the target variable 'Vegetation_Type'\n",
    "for i, col in enumerate(numerical_columns):\n",
    "    row = i // cols\n",
    "    col_idx = i % cols\n",
    "    sns.violinplot(x=df['Vegetation_Type'], y=df[col], ax=axes[row, col_idx], palette='Set2')  # Vegetation_Type on x-axis\n",
    "    axes[row, col_idx].set_title(f'Vegetation_Type vs {col}')\n",
    "    axes[row, col_idx].set_xlabel('Vegetation_Type')  \n",
    "    axes[row, col_idx].set_ylabel(col)  \n",
    "\n",
    "# Hide any unused subplots (in case the grid has extra slots)\n",
    "if num_plots % cols != 0:  # Only if there are unused subplots\n",
    "    for j in range(num_plots, rows * cols):\n",
    "        fig.delaxes(axes.flat[j])\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62f4842-cb65-478c-94fd-814b50cdb3bf",
   "metadata": {},
   "source": [
    "From these violinplots, some general observations can be made.\n",
    "\n",
    "**Altitude**: The vegetation types appear to be well-separated by altitude. Type_1 is at a higher altitude, while Type_3 is at a lower altitude, with Type_2 in between. This suggests altitude may be a good feature for distinguishing between the types.\n",
    "\n",
    "**Slope_Orientation**: All three types show overlap in terms of slope orientation, so it doesn't seem to differentiate vegetation types strongly.\n",
    "\n",
    "**Slope**: There is no significant distinction between the vegetation types based on slope alone, as all seem to occupy similar ranges.\n",
    "\n",
    "**Horizontal and Vertical Distance to Water**: These variables show some degree of separation, especially for Type_3, which tends to have smaller horizontal distances to water. Type_1 and Type_2 overlap more but still show some separation.\n",
    "\n",
    "**Shadow Index (9h, 12h, 15h)**: Thereâ€™s a fair amount of overlap in the shadow indices among the vegetation types, meaning these variables may not be significant in distinguishing between them.\n",
    "\n",
    "**Horizontal Distance to Roadways**: This feature appears to be quite distinct for Type_3, which has a smaller range and distances from roadways compared to Type_2 and Type_3.\n",
    "\n",
    "**Horizontal Distance to Fire Points**: This variable has some separation between vegetation types, with Type_3 having much lower distances to fire points than Type_1 and Type_2, which have a wider distribution.\n",
    "\n",
    "**Canopy Density**: All vegetation types appear to have similar canopy densities, making it difficult to differentiate between them based on this feature.\n",
    "\n",
    "**Rainfall (Summer and Winter)**: The rainfall in both seasons seems to be very similar across vegetation types, showing little to no variation or overlap.\n",
    "\n",
    "**Wind Exposure Level**: There is minimal distinction among the vegetation types based on wind exposure, as all three distributions seem similar.\n",
    "\n",
    "**Key Insights**:\n",
    "Altitude and Horizontal Distance to Roadways/Fire Points appear to be strong variables for separating vegetation types. Some features, like Slope Orientation, Shadow Indices, and Canopy Density, show a lot of overlap, suggesting they might not be as important in classification.\n",
    "Horizontal Distance to Water also provides some separability for Type_3, which might help in classification.  \n",
    "\n",
    "To gain more insights in the correlation between the numerical features, a heatmap is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1adfeb8-89fd-4dab-8448-f29a3e33b4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use factorize to define the categorical target variable into classes\n",
    "df['Vegetation_Type'] = df.Vegetation_Type.factorize()[0]\n",
    "\n",
    "# List of numerical columns to include in the correlation heatmap\n",
    "numerical_columns = [\n",
    "    'Altitude', 'Slope_Orientation', 'Slope', \n",
    "    'Horizontal_Distance_To_Water', 'Vertical_Distance_To_Water', \n",
    "    'Horizontal_Distance_To_Roadways', 'Shadow_Index_9h', \n",
    "    'Shadow_Index_12h', 'Shadow_Index_15h', \n",
    "    'Horizontal_Distance_To_Fire_Points', 'Canopy_Density', \n",
    "    'Rainfall_Summer', 'Rainfall_Winter', 'Wind_Exposure_Level',\n",
    "    'Vegetation_Type'  \n",
    "]\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr_matrix = df[numerical_columns].corr()\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Correlation Heatmap of Numerical Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764fd53a-1388-4696-bd70-1b3f396496e9",
   "metadata": {},
   "source": [
    "Some observations can be made from this Correlation Heatmap.\n",
    "\n",
    "**Altitude**: Strong negative correlation with Vegetation_Type (-0.85), showing altitude plays a major role in distinguishing vegetation types.\n",
    "\n",
    "**Horizontal_Distance_To_Roadway**s: Moderate negative correlation (-0.44), indicating distance from roadways helps differentiate vegetation types.\n",
    "\n",
    "**Slope**: Positive correlation (0.37), suggesting steeper slopes are more common in certain vegetation types.\n",
    "\n",
    "**Horizontal_Distance_To_Fire_Points**: Moderate negative correlation (-0.35), showing vegetation type is influenced by distance from fire points.\n",
    "\n",
    "**Shadow Indices**: Little correlation with Vegetation_Type, implying minimal impact on vegetation classification.  \n",
    "\n",
    "Some Bivariate analysis can be done on the categorical variables as well. Using stacked barcharts the distribution of the categorical variables per vegetation type can be visualized. In the graphs, the values of Vegetation_Type 0, 1 and 2 correspond to respectively Type_1, Type_2 and Type_3. As can be seen, the distribution of the *Vegetation_Type* differs highly per category of the *Soil_Type* and *Wilderness_Area*. For example, Wilderness Area 4 almost always has Vegetation Type 3. This indicates that both *Soil_Type* and *Wilderness_Area* can be relevant features in the classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f94a86-521f-4b6b-b3df-650388c0191d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the categorical column names\n",
    "categorical_columns = [\n",
    "    'Soil_Type', 'Wilderness_Area']\n",
    "\n",
    "# Set up subplots\n",
    "fig, axes = plt.subplots(1, len(categorical_columns), figsize=(18, 6))\n",
    "fig.suptitle('Categorical Variables vs Vegetation_Type', fontsize=16)\n",
    "\n",
    "# Plot stacked bar charts for each categorical feature\n",
    "for i, col in enumerate(categorical_columns):\n",
    "    # Create a cross-tabulation of 'Vegetation_Type' against the categorical variable\n",
    "    crosstab = pd.crosstab(df[col], df['Vegetation_Type'], normalize='index')\n",
    "    \n",
    "    # Plot the stacked bar chart\n",
    "    crosstab.plot(kind='bar', stacked=True, ax=axes[i], colormap='Set2')\n",
    "    axes[i].set_title(f'{col} vs Vegetation_Type')\n",
    "    axes[i].set_ylabel('Proportion')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749ac564-d7f5-48af-b731-20ea9cce4e51",
   "metadata": {},
   "source": [
    "**Summarized**:  \n",
    "From this Univariate and Bivariate analysis some information can be extracted before applying any Machine Learning model. The features that are very relevant in the classification problem of the vegetation types are the altitude, slope, distances to roadways/fire points, soil type and wilderness area. On the other hand, the shadow indices seem to be of little importance in applying Machine Learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4a3b60-8cf1-4998-9c45-957aa5974de7",
   "metadata": {},
   "source": [
    "### Machine Learning Methods \n",
    "\n",
    "**Data Preprocessing**  \n",
    "Before Machine Learning models can be applied to the data, the data first needs some preprocessing. First of all, as most Machine Learning models can only handle numerical data, the categorical variables should be transformed into numerical variables. Several methods exist for this, but since our categorical data has no order, the most suitable method is OneHotEncoding. OneHotEncoding creates a new binary column for every class in the categorical columns. OneHotEncoding can be done fast using the built-in pandas function *get_dummies()*.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5e6b60-54a4-42f8-9edb-c495f76d5720",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, columns=['Soil_Type', 'Wilderness_Area'], drop_first=True)  # drop_first to avoid dummy variable trap\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92664744-e682-4841-8f5c-1322ed77c817",
   "metadata": {},
   "source": [
    "Moreover, another data preprocessing step is scaling. Scaling must happen to make sure that no features become more dominant only because they are larger in magnitude. Several methods exist for data scaling such as MinMax Scaling and Standardization. Since the features have different distributions, Standardization is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0d6de42a-0aa4-41c1-96fd-fac1e6faf121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into features (X) and target (y)\n",
    "X = df.drop(['Id', 'Vegetation_Type'], axis=1)  # independent variables, drop Id and the target variable\n",
    "y = df['Vegetation_Type']  # target variable\n",
    "\n",
    "# Apply scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)  # scale the input data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee9eaa8-3a87-45e4-951e-b55e62a5b132",
   "metadata": {},
   "source": [
    "#### Code multiple resampling methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7305c5ed-176a-4223-bff7-a494e44c8457",
   "metadata": {},
   "source": [
    "We will only give the code for LOOCV and not run it since it is very computationally expensive and a good method for small datasets. However, we have quite a big dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "74aa9517-cab8-4b6a-8dd0-9d27bc54650e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.model_selection import KFold, cross_val_score, LeaveOneOut\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Holdout resampling\n",
    "def holdout(X, y, test_size=0.3, random_state=42):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state) \n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# k-fold cross-validation\n",
    "def k_fold_cv(model, X, y, k, random_state=42):\n",
    "\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=random_state)\n",
    "    cv_results = cross_val_score(model, X, y, cv=kf, scoring='accuracy')\n",
    "    \n",
    "    print(f\"Cross-Validation Accuracies: {cv_results}\")\n",
    "    mean_accuracy = cv_results.mean()\n",
    "    print(f\"Mean Accuracy: {mean_accuracy:.2f}\")\n",
    "    \n",
    "    return mean_accuracy\n",
    "\n",
    "# If the sample is small, you can use LOOCV. Here is the code but the sample is not small so probably we won't use it\n",
    "def LOOCV(model, X, y):\n",
    "    # Initialize Leave-One-Out Cross-Validation\n",
    "    loo = LeaveOneOut()\n",
    "\n",
    "    # Perform LOOCV\n",
    "    cv_results = cross_val_score(model, X, y, cv=loo, scoring='accuracy')\n",
    "\n",
    "    return cv_results\n",
    "\n",
    "def bootstrap(X, y, n_iterations=10):\n",
    "    n_samples = X_scaled.shape[0]\n",
    "    accuracies = []\n",
    "\n",
    "    # Loop over the number of bootstrap iterations\n",
    "    for i in range(n_iterations):\n",
    "        # Create a bootstrap sample\n",
    "        indices = np.random.choice(range(n_samples), size=n_samples, replace=True)\n",
    "        X_train, y_train = X.iloc[indices], y.iloc[indices]\n",
    "        \n",
    "        # Find the out-of-bag samples (those not in the bootstrap sample)\n",
    "        out_of_bag = list(set(range(n_samples)) - set(indices))\n",
    "        X_test, y_test = X.iloc[out_of_bag], y.iloc[out_of_bag]\n",
    "        \n",
    "        # Initialize the logistic regression model\n",
    "        model = LogisticRegression()\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluate on the out-of-bag data\n",
    "        if len(out_of_bag) > 0:  # Ensure there are out-of-bag samples\n",
    "            y_pred = model.predict(X_test)\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            accuracies.append(accuracy)\n",
    "    \n",
    "    # Return the list of accuracies from all iterations\n",
    "    return accuracies\n",
    "\n",
    "\n",
    "# make ROC curves\n",
    "# make plot of training and test validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0f8950-ebb1-40eb-9182-da45cc514c48",
   "metadata": {},
   "source": [
    "#### Evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd6f54b-0c85-478f-ba30-820653035e3e",
   "metadata": {},
   "source": [
    "Define and give some theory on which evaluation metrics we will use and why we use them. R2-score, accuracy and MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8653383f-0126-4f55-a794-f5b1194d0da5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c1a17a1-47f5-444c-a1f8-2d95d5694733",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b499cb16-a212-45cc-aeb1-3a2416c79a34",
   "metadata": {},
   "source": [
    "We can use Linear Regression in classification problems. However, LR can result in probabilities for belonging to a certain class of below 0 and above 1. Logistic Regression makes sure that these probabilities are between 0 and 1 using a logistic function. \n",
    "\n",
    "### Overall Insights\n",
    "\n",
    "- The model achieved a commendable accuracy of 84.68%, indicating a strong performance in classifying the vegetation types.\n",
    "- **Type 3** demonstrated the best performance, with high precision, recall, and F1-scores, suggesting the model effectively distinguishes this class from others.\n",
    "- **Type 2** exhibited lower scores compared to the other types, indicating potential challenges in correctly classifying this class, which could warrant further investigation into feature selection or model tuning.\n",
    "\n",
    "Overall, the logistic regression model provides a good foundation for predicting vegetation types based on the features used.\n",
    "\n",
    "#### This is without reasempling methods, so we can compare it in the end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f91e595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target variable\n",
    "y = df['Vegetation_Type']\n",
    "\n",
    "# Define feature set (X), excluding the target column ('Vegetation_Type')\n",
    "X = df.drop(['Vegetation_Type', 'Id'], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature Scaling (Standardize the data)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "logreg = LogisticRegression(max_iter=1000)  \n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n",
    "\n",
    "# Step 8: Visualize the confusion matrix\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual Values')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f94ba5",
   "metadata": {},
   "source": [
    "### Description of the Plot\n",
    "- **Scatter Points**:\n",
    "  - The scatter plot contains three distinct sets of points, each representing different classes. \n",
    "  - Points corresponding to Class 0 and Class 1 are clearly visible, while there is a noticeable absence of points representing Class 2 in the area where Class 1 is prevalent. \n",
    "\n",
    "- **Class Probability Curves**:\n",
    "  - There are three probability curves plotted, corresponding to the classes:\n",
    "    - **Curve 0**: This curve represents the predicted probability of Class 0. As \"Altitude\" increases, the probability of Class 0 decreases.\n",
    "    - **Curve 1**: This curve shows the predicted probability for Class 1. It increases as \"Altitude\" rises, indicating a positive relationship between \"Altitude\" and the likelihood of belonging to Class 1.\n",
    "    - **Class 2**: There is no curve for Class 2 against Class 1. This suggests that Class 2 may not have a significant relationship with \"Altitude\" in this dataset, as no probabilities are predicted for it.\n",
    "\n",
    "### Observations\n",
    "- **Class Separation**:\n",
    "  - The curves for Class 0 and Class 1 show a clear separation based on \"Altitude.\" As altitude increases, the probability of Class 0 decreases while the probability of Class 1 increases. This indicates that higher altitudes are associated with a higher likelihood of belonging to Class 1.\n",
    "  - There is a lack of overlap between Class 2 and the other two classes, suggesting that Class 2 may be an outlier or has no representation in the altitude range observed.\n",
    "\n",
    "- **Model Fit**:\n",
    "  - The model appears to fit the data points of Classes 0 and 1 well, with the probability curves closely following the distribution of the points. However, the absence of points for Class 2 raises questions about its relevance in this context.\n",
    "  \n",
    "- **Uncertainty Regions**:\n",
    "  - The area where the probabilities hover around 0.5 seems to be absent, indicating that the model is confident in its predictions for Classes 0 and 1 across the range of \"Altitude.\" This lack of uncertainty might suggest a strong relationship between \"Altitude\" and the presence of Class 1, whereas Class 2 is not well-represented.\n",
    "\n",
    "\n",
    "# TODO FIX THIS TEXT NOT SURE IF IT IS THAT TRUE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3880380c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_altitude = df[['Altitude']]\n",
    "feature_name = X_altitude.columns[0]  \n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_altitude)\n",
    "\n",
    "logreg = LogisticRegression(multi_class='ovr', max_iter=1000)\n",
    "logreg.fit(X_scaled, y)\n",
    "\n",
    "X_test = np.linspace(X_altitude[feature_name].min(), X_altitude[feature_name].max(), 300).reshape(-1, 1)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "y_prob = logreg.predict_proba(X_test_scaled)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(X_altitude[feature_name], y, color='orange', edgecolor='k', alpha=0.7, label='Data points', s=50)\n",
    "\n",
    "for i in range(y_prob.shape[1]):\n",
    "    plt.plot(X_test, y_prob[:, i], label=f'Class {i} Probability')\n",
    "\n",
    "plt.xlabel(feature_name)\n",
    "plt.ylabel('Probability')\n",
    "plt.title(f'Logistic Regression Fit for {feature_name}')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db988e6f",
   "metadata": {},
   "source": [
    "The model is robust overall, achieving an accuracy of 84.57%. I was trying to drop different features according to graphs and heatmap to try if i will have better result without all features and it didnt help me, but still it is hard to train data for type 2 because in most of the features its simillar with type 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e931c69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment when u wanna try but if u run codes under it comment it and run again because it drops columns!\n",
    "X_dropped = X.drop(['Rainfall_Summer', 'Rainfall_Winter', 'Wind_Exposure_Level', 'Canopy_Density', 'Shadow_Index_9h'], axis=1)\n",
    "\n",
    "X_train_dropped, X_test_dropped, y_train, y_test = train_test_split(X_dropped, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler_dropped = StandardScaler()\n",
    "X_train_dropped = scaler_dropped.fit_transform(X_train_dropped)\n",
    "X_test_dropped = scaler_dropped.transform(X_test_dropped)\n",
    "\n",
    "# Train Logistic Regression model on dropped feature set\n",
    "logreg_dropped = LogisticRegression(max_iter=1000)  \n",
    "logreg_dropped.fit(X_train_dropped, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_dropped = logreg_dropped.predict(X_test_dropped)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_dropped = accuracy_score(y_test, y_pred_dropped)\n",
    "conf_matrix_dropped = confusion_matrix(y_test, y_pred_dropped)\n",
    "class_report_dropped = classification_report(y_test, y_pred_dropped)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Accuracy (after dropping features): {accuracy_dropped}\")\n",
    "print(\"Confusion Matrix (after dropping features):\")\n",
    "print(conf_matrix_dropped)\n",
    "print(\"Classification Report (after dropping features):\")\n",
    "print(class_report_dropped)\n",
    "\n",
    "# Visualize the confusion matrix\n",
    "sns.heatmap(conf_matrix_dropped, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix (after dropping features)')\n",
    "plt.ylabel('Actual Values')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918917c3",
   "metadata": {},
   "source": [
    "## LDA \n",
    "The Linear Discriminant Analysis (LDA) model achieved an overall accuracy of approximately 83.51% after dropping certain features. While the model performed well in predicting Class 0 and Class 2, there is room for improvement in Class 1, where both precision and recall are lower. Overall, the model demonstrates good performance. I used it with dropped features because in this method i had better accuracy when i dropped them.\n",
    "\n",
    "But its also not optimal for our dataset because it should have better performance when features are `normally` distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c750fe5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Assuming df is your DataFrame and y is the target variable\n",
    "y = df['Vegetation_Type']\n",
    "X = df.drop(['Vegetation_Type', 'Id'], axis=1)\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature Scaling\n",
    "scaler_dropped = StandardScaler()\n",
    "X_train_dropped = scaler_dropped.fit_transform(X_train)\n",
    "X_test_dropped = scaler_dropped.transform(X_test)\n",
    "\n",
    "# Initialize and fit the LDA model\n",
    "lda_dropped = LinearDiscriminantAnalysis()\n",
    "lda_dropped.fit(X_train_dropped, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_dropped = lda_dropped.predict(X_test_dropped)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_dropped = accuracy_score(y_test, y_pred_dropped)\n",
    "conf_matrix_dropped = confusion_matrix(y_test, y_pred_dropped)\n",
    "class_report_dropped = classification_report(y_test, y_pred_dropped)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Accuracy (after dropping features): {accuracy_dropped}\")\n",
    "print(\"Confusion Matrix (after dropping features):\")\n",
    "print(conf_matrix_dropped)\n",
    "print(\"Classification Report (after dropping features):\")\n",
    "print(class_report_dropped)\n",
    "\n",
    "# Visualize the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_dropped, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix (after dropping features)')\n",
    "plt.ylabel('Actual Values')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6b43a5",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this analysis, we utilized Linear Discriminant Analysis (LDA) to visualize the decision boundaries of our model based on selected features from the dataset. Specifically, we focused on the 14th column and the last column of the dataset.\n",
    "\n",
    "The resulting plot illustrated how the LDA model distinguishes between different classes based on the values of the chosen features. The color gradient in the background represents the predicted class regions, while the scatter plots indicate the actual training and test data points. \n",
    "\n",
    "- **Training Data**: Shown with circular markers, they indicate the distribution of the classes within the training dataset.\n",
    "- **Test Data**: Represented by cross markers, they provide insight into how well the model predicts unseen data.\n",
    "\n",
    "The decision boundaries reflect the relationships between the selected features and their influence on class separation. Overall, the visualization highlights the effectiveness of LDA in classifying the data based on the chosen features, as well as the potential areas of overlap where class distinctions may be less clear.\n",
    "\n",
    "## CHECK WITH TEACHER IF THIS GRAPH IS EVEN OK?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f10a58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vis = X_train_dropped[:, [13, -1]]\n",
    "\n",
    "feature_names = X.columns[[13, -1]]\n",
    "\n",
    "print(\"Selected features for visualization (14th column and last column):\")\n",
    "print(feature_names.tolist())\n",
    "print(X_vis)\n",
    "\n",
    "lda_dropped_vis = LinearDiscriminantAnalysis()\n",
    "lda_dropped_vis.fit(X_vis, y_train)\n",
    "\n",
    "x_min, x_max = X_vis[:, 0].min() - 1, X_vis[:, 0].max() + 1\n",
    "y_min, y_max = X_vis[:, 1].min() - 1, X_vis[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                     np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "Z = lda_dropped_vis.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "contour = plt.contourf(xx, yy, Z, alpha=0.5, cmap='Spectral')\n",
    "\n",
    "cbar = plt.colorbar(contour)\n",
    "cbar.set_label('Predicted Class', rotation=270, labelpad=15)\n",
    "\n",
    "plt.scatter(X_vis[:, 0], X_vis[:, 1], c=y_train, edgecolor='k', s=120, cmap='Spectral', marker='o', label='Training Data', alpha=0.9)\n",
    "\n",
    "X_test_vis = X_test_dropped[:, [13, -1]]\n",
    "plt.scatter(X_test_vis[:, 0], X_test_vis[:, 1], c=y_test, edgecolor='k', marker='x', s=120, label='Test Data', alpha=0.2)\n",
    "\n",
    "plt.title('LDA Decision Boundaries')\n",
    "plt.xlabel(feature_names[0])\n",
    "plt.ylabel(feature_names[1])\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6527afa",
   "metadata": {},
   "source": [
    "# QDA\n",
    "QDA is a versatile classification method that shines in scenarios where non-linear decision boundaries, class-specific variances, and multi-class situations are present. Its ability to model complex relationships and robustness in handling class imbalances makes it a valuable tool in the data scientist's toolkit.\n",
    "\n",
    "In our dataset we can see its not that good for using.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ac6b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Assuming df is your DataFrame and y is the target variable\n",
    "y = df['Vegetation_Type']\n",
    "X = df.drop(['Vegetation_Type', 'Id'], axis=1)\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature Scaling\n",
    "scaler_dropped = StandardScaler()\n",
    "X_train_dropped = scaler_dropped.fit_transform(X_train)\n",
    "X_test_dropped = scaler_dropped.transform(X_test)\n",
    "\n",
    "# Initialize and fit the LDA model\n",
    "qda_dropped = QuadraticDiscriminantAnalysis()\n",
    "qda_dropped.fit(X_train_dropped, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_dropped = qda_dropped.predict(X_test_dropped)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_dropped = accuracy_score(y_test, y_pred_dropped)\n",
    "conf_matrix_dropped = confusion_matrix(y_test, y_pred_dropped)\n",
    "class_report_dropped = classification_report(y_test, y_pred_dropped)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Accuracy (after dropping features): {accuracy_dropped}\")\n",
    "print(\"Confusion Matrix (after dropping features):\")\n",
    "print(conf_matrix_dropped)\n",
    "print(\"Classification Report (after dropping features):\")\n",
    "print(class_report_dropped)\n",
    "\n",
    "# Visualize the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_dropped, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix (after dropping features)')\n",
    "plt.ylabel('Actual Values')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14c3809-a3fc-47b4-9588-c7dcfd669a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the logistic regression model\n",
    "log_reg = LogisticRegression()\n",
    "\n",
    "X_train, X_test, y_train, y_test = holdout(X_scaled, y, test_size=0.3)\n",
    "k_fold_cv(log_reg, X_scaled, y, 5)\n",
    "#LOOCV(log_reg, X_scaled, y) --> takes very long since dataset is quite big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef4e79b-070b-4d9c-8b9c-d43dfc198b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Print results\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)\n",
    "print('Classification Report:')\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6ea98428-668f-4f48-83f0-ab801ae0af9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a81072-e440-42cd-85fb-b819d2e39659",
   "metadata": {},
   "source": [
    "#### Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7aad18-abdb-44cf-812e-e729aa2f85dc",
   "metadata": {},
   "source": [
    "explain when to use LDA and if this applies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97765df6-6250-446b-9ee0-1e9375622731",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "\n",
    "# Build the linear discriminant analysis model\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "\n",
    "# Train the model\n",
    "lda.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_lda = lda.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred_lda)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_lda)\n",
    "class_report = classification_report(y_test, y_pred_lda)\n",
    "\n",
    "# Print results\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)\n",
    "print('Classification Report:')\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63369eb-f58b-4f7a-8bd4-275801df175b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "948c295a-112e-4e8a-9069-eec2eb7997c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b896c0de-108f-458d-8238-3afe250917a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c04ef572-df53-45e7-b76c-af94ff1113e8",
   "metadata": {},
   "source": [
    "#### Quadrant Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333b4c18-c849-4d4f-a8eb-5c0ea0ecb12e",
   "metadata": {},
   "source": [
    "explain when to use QDA and evaluate if this is necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66821237-fa36-474b-9d7c-225f3defd07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the linear discriminant analysis model\n",
    "qda = QuadraticDiscriminantAnalysis()\n",
    "\n",
    "# Train the model\n",
    "qda.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_qda = qda.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred_qda)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_qda)\n",
    "class_report = classification_report(y_test, y_pred_qda)\n",
    "\n",
    "# Print results\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)\n",
    "print('Classification Report:')\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b9e31b-d342-49c0-83b2-c2c619cb9087",
   "metadata": {},
   "source": [
    "### LDA vs QDA --> check covariance matrices of classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dde5513-5557-4a51-aa90-40b04bfd68e1",
   "metadata": {},
   "source": [
    "If the covariance matrices are similar across classes, LDA should work well; if they are different, QDA is likely the better choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93ed623-a2a6-42df-934e-aad6a919c5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = df['Vegetation_Type'].unique()\n",
    "df_cov = df.drop(['Id', 'Soil_Type', 'Wilderness_Area'], axis=1)  # independent variables\n",
    "covariances = {}\n",
    "\n",
    "for class_value in classes:\n",
    "    class_data = df_cov[df_cov['Vegetation_Type'] == class_value].drop('Vegetation_Type', axis=1)  # Drop target column\n",
    "    covariances[class_value] = np.cov(class_data, rowvar=False)\n",
    "\n",
    "# Output covariance matrices for each class\n",
    "for class_value, cov_matrix in covariances.items():\n",
    "    print(f\"Covariance matrix for class {class_value}:\\n\", cov_matrix, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe5ae74-a8fa-4154-a7f7-3aab0ffebaba",
   "metadata": {},
   "source": [
    "#### Feature selection using regularization methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35573d88-d9ad-4458-8a2d-f44d670a7dfa",
   "metadata": {},
   "source": [
    "Removing features can improve understandability and performance of the model. Feature selection can be done automatically Ridge, Lasso or Elastic Net Regression. When coefficients of variables become 0 or increasingly small, these features can be removed from the classification model to improve model interpretability and performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f25197b-93bf-4513-ae5f-f3ff19040502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Ridge regression model\n",
    "ridge = Ridge()\n",
    "\n",
    "# Set up a grid of alpha values to try\n",
    "alpha_values = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100]}  # You can adjust this range\n",
    "\n",
    "# Set up GridSearchCV to tune alpha\n",
    "grid_search = GridSearchCV(ridge, param_grid=alpha_values, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "\n",
    "# Fit the model to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model (best alpha)\n",
    "best_ridge = grid_search.best_estimator_\n",
    "print(f\"Best alpha value: {grid_search.best_params_['alpha']}\")\n",
    "\n",
    "# Predict on the test set using the best model\n",
    "y_pred = best_ridge.predict(X_test)\n",
    "\n",
    "# Evaluate the model using Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error with best alpha: {mse}\")\n",
    "\n",
    "# Print the coefficients along with the corresponding feature names\n",
    "coefficients = best_ridge.coef_\n",
    "feature_names = X.columns\n",
    "\n",
    "# Combine the feature names and their corresponding coefficients\n",
    "coeff_df = pd.DataFrame({'Feature': feature_names, 'Ridge Coefficient': coefficients})\n",
    "\n",
    "# Display the coefficients\n",
    "print(\"\\nRidge Coefficients with best alpha:\")\n",
    "print(coeff_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66eb8d37-c41b-4cae-98cb-95a75b98783a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6df807-90ca-412a-be2e-ddd7316127c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Lasso regression model\n",
    "lasso = Lasso()\n",
    "\n",
    "# Set up a grid of alpha values to try\n",
    "alpha_values = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100]}  # You can adjust this range\n",
    "\n",
    "# Set up GridSearchCV to tune alpha\n",
    "grid_search = GridSearchCV(lasso, param_grid=alpha_values, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "\n",
    "# Fit the model to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model (best alpha)\n",
    "best_lasso = grid_search.best_estimator_\n",
    "print(f\"Best alpha value: {grid_search.best_params_['alpha']}\")\n",
    "\n",
    "# Predict on the test set using the best model\n",
    "y_pred = best_lasso.predict(X_test)\n",
    "\n",
    "# Evaluate the model using Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error with best alpha: {mse}\")\n",
    "\n",
    "# Print the coefficients along with the corresponding feature names\n",
    "coefficients = best_lasso.coef_\n",
    "feature_names = X.columns\n",
    "\n",
    "# Combine the feature names and their corresponding coefficients\n",
    "coeff_df = pd.DataFrame({'Feature': feature_names, 'Lasso Coefficient': coefficients})\n",
    "\n",
    "# Display the coefficients\n",
    "print(\"\\nLasso Coefficients with best alpha:\")\n",
    "print(coeff_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c474c77a-d182-4aed-a254-2c4fec891034",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b772cc5e-f737-4555-95b4-744944bb96fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try ridge regression with multiple alphas, make a plot of the coefficients and the L2 norm\n",
    "# make plot of bias-variance trade-off and use CV for selecting alpha\n",
    "lambdas = 10**np.linspace(10, -2, 100)*0.5\n",
    "\n",
    "ridge = Ridge()\n",
    "coefs = []\n",
    "\n",
    "for l in lambdas:\n",
    "    ridge.set_params(alpha = l)\n",
    "    ridge.fit(X_scaled, y)\n",
    "    coefs.append(ridge.coef_)\n",
    "\n",
    "np.shape(coefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259ca0d8-2324-4b20-84ed-7a63133e9af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the coefficients as they change over values of lambda\n",
    "ax = plt.gca()\n",
    "ax.plot(lambdas, coefs)\n",
    "ax.set_xscale('log')\n",
    "plt.axis('tight')\n",
    "plt.xlabel('lambda')\n",
    "plt.ylabel('weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37a9327-fcdc-47e5-afad-f50a37357db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge2 = Ridge(alpha = 0.0000001)\n",
    "ridge2.fit(X_train, y_train)\n",
    "pred2 = ridge2.predict(X_test)\n",
    "print(pd.Series(ridge2.coef_))\n",
    "print(mean_squared_error(y_test, pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8917bd3-be56-4780-baf1-697354fd53f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridgecv = RidgeCV(alphas = lambdas, scoring = 'neg_mean_squared_error')\n",
    "ridgecv.fit(X_train, y_train)\n",
    "ridgecv.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbafdf0a-fb98-4991-9334-760938ee8c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge4 = Ridge(alpha = ridgecv.alpha_)\n",
    "ridge4.fit(X_train, y_train)\n",
    "mean_squared_error(y_test, ridge4.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0862bff5-e61a-4623-9a01-2e1766acec18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a bias-variance trade-off plot\n",
    "bias_sq = []\n",
    "variance = []\n",
    "mse = []\n",
    "\n",
    "n_splits = 10\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "for l in lambdas:\n",
    "    y_preds = np.zeros((X_scaled.shape[0], n_splits))\n",
    "\n",
    "    for fold_idx, (train_idx, test_idx) in enumerate(kf.split(X_scaled)):\n",
    "        X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        ridge.set_params(alpha = l)\n",
    "        ridge.fit(X_train, y_train)\n",
    "\n",
    "        y_preds[test_idx, fold_idx] = ridge.predict(X_test).flatten()\n",
    "\n",
    "    avg_preds = np.mean(y_preds, axis=1)\n",
    "    bias_sq.append(np.mean((avg_preds - y.to_numpy().flatten())**2))\n",
    "    variance.append(np.mean(np.var(y_preds, axis=1)))\n",
    "    mse.append(bias_sq[-1] + variance[-1])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(lambdas, bias_sq, label=\"Bias^2\", color=\"blue\", linewidth=2)\n",
    "plt.plot(lambdas, variance, label=\"Variance\", color=\"green\", linewidth=2)\n",
    "plt.plot(lambdas, mse, label=\"Total MSE\", color=\"red\", linestyle='--', linewidth=2)\n",
    "plt.xscale('log')  # Use log scale for lambda\n",
    "plt.xlabel(\"Lambda (alpha)\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.title(\"Bias-Variance Trade-off in Ridge Regression (Cross-Validation)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828953e2-3d75-4d21-827c-cbbb2d0b62a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7470dff5-21f2-4deb-a384-268d9777b6cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae02df32-6dbf-4a03-9ec0-bc9691cd8fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso(max_iter = 10000)\n",
    "coefs_lasso = []\n",
    "\n",
    "for l in lambdas:\n",
    "    lasso.set_params(alpha = l)\n",
    "    lasso.fit(X_scaled, y)\n",
    "    coefs_lasso.append(lasso.coef_)\n",
    "\n",
    "# Plot the coefficients as they change over values of lambda\n",
    "ax = plt.gca()\n",
    "ax.plot(lambdas, coefs_lasso)\n",
    "ax.set_xscale('log')\n",
    "plt.axis('tight')\n",
    "plt.xlabel('lambda')\n",
    "plt.ylabel('weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768f3876-83ea-4715-8b86-b4cb8a92a2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "lassocv = LassoCV(alphas = None, cv = 10, max_iter = 100000)\n",
    "lassocv.fit(X_train, y_train)\n",
    "\n",
    "lasso.set_params(alpha=lassocv.alpha_)\n",
    "lasso.fit(X_train, y_train)\n",
    "mean_squared_error(y_test, lasso.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29676df5-5747-441c-a7ae-b0269a858a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_test,  lasso.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36e2b93-60dd-4325-8648-5b493a1a68fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(lasso.coef_, index=X.columns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a853bf2f-9890-4b11-afa1-5c1660b9b1c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e34f84a",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "There are two major problems related to training models: **overfitting** and **underfitting**:\n",
    "\n",
    "- Overfitting: The model performs well on the training set but not so well on unseen (test) data.\n",
    "- Underfitting: Neither performs well on the train set nor on the test set.\n",
    "\n",
    "**Regularization** is implemented to avoid overfitting of the data, especially when there is a large variance between train and test set\n",
    "performances. There are different methods of reducing the model complexity and preventing overfitting in linear models which are *Ridge* and *Lasso*\n",
    "*Regression Models*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097bc7c9",
   "metadata": {},
   "source": [
    "## Preparing Data for Ridge Regression\n",
    "\n",
    "Before applying Ridge Regression, we need to properly prepare the dataset. This involves several steps, such as:\n",
    "\n",
    "1. **Feature Selection/Engineering**: Ensure relevant features are selected and encoded.\n",
    "2. **Splitting the Data**: Divide the dataset into training and test sets.\n",
    "3. **Feature Scaling**: Apply scaling to ensure features are on a similar scale, as Ridge Regression is sensitive to feature magnitudes.\n",
    "\n",
    "Below is an example of how to prepare data for Ridge Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a4e9d0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all libraries \n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet \n",
    "from sklearn.metrics import mean_squared_error , root_mean_squared_error\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler \n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "def prepare_dataset(df, target, categorical_features, numerical_features, numberical_target_var = True):\n",
    "\n",
    "    df.isnull().sum()*100/df.shape[0]\n",
    "    df.head()\n",
    "    # Encode the target variable if it's categorical\n",
    "    if numberical_target_var == False:\n",
    "        le = LabelEncoder()\n",
    "        y = le.fit_transform(df[target])\n",
    "    \n",
    "    # Convert the categorical columns into the boolean type, remove the Id col\n",
    "    dummies = pd.get_dummies(df[categorical_features])\n",
    "\n",
    "    # Drop the categorical features from the X\n",
    "    X_ = df.drop(categorical_features, axis = 1) \n",
    "    # Drop the target variable\n",
    "    X_ = X_.drop(target, axis = 1) \n",
    "    X_ = X_.drop([\"Id\"], axis=1) # TODO remove\n",
    "\n",
    "    # Convert all numerical values to the float type\n",
    "    X = X_.apply(lambda col: col.astype('float64') if col.dtype in ['int64', 'float32', 'float64'] else col)\n",
    "    # Merge the dummies (categorical var w numerical variables)\n",
    "    X = pd.concat([X, dummies], axis = 1) \n",
    "\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b3065b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the head of the dataset before any modification\n",
    "# df = df.drop(\"Vegetation_Type_Encoded\", axis=1)\n",
    "df = pd.concat([df_1, df_2, df_3], ignore_index=True)\n",
    "df.head(5)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99299ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the column with the independent variable (Salary), and columns for which we created dummy variables \n",
    "y = df.Vegetation_Type\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "numerical_features = [\"Altitude\", \"Slope_Orientation\", \"Slope\", \"Horizontal_Distance_To_Water\"\n",
    "    ,\"Vertical_Distance_To_Water\", \"Horizontal_Distance_To_Roadways\", \"Shadow_Index_9h\",\n",
    "    \"Shadow_Index_12h\", \"Shadow_Index_15h\", \"Horizontal_Distance_To_Fire_Points\", \n",
    "    \"Canopy_Density\", \"Rainfall_Summer\", \"Rainfall_Winter\", \"Wind_Exposure_Level\"]\n",
    "categorical_features = [\"Wilderness_Area\", \"Soil_Type\"]\n",
    "target_variable = \"Vegetation_Type\"\n",
    "\n",
    "print(df.columns)\n",
    "\n",
    "# Get x,y data\n",
    "X, y = prepare_dataset(df, target_variable, categorical_features, numerical_features, False)\n",
    "# Show info about X data\n",
    "X.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ae0ddf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "sc=StandardScaler() \n",
    "X_train=sc.fit_transform(X_train) \n",
    "X_test=sc.transform(X_test)\n",
    "\n",
    "#from sklearn.preprocessing import minmax_scale \n",
    "# minmax_scale = MinMaxScaler()\n",
    "# X_train=minmax_scale(X_train, axis=0) \n",
    "# X_test=sc.transform(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "807447d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate an NumPy array of alphas from small to large \n",
    "# number which are logarithmically spaced, meaning they decrease exponentially\n",
    "\n",
    "alphas = 10**np.linspace(10,-2,100)*0.5 \n",
    "\n",
    "# Uncomment the alphas if you want to see the array\n",
    "# alphas "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95b735e",
   "metadata": {},
   "source": [
    "## Ridge Resgression\n",
    "Is a variation of linear regression, specifically designed to address multicollinearity in the dataset. In linear regression, the goal is to find the best-fitting hyperplane that minimizes the sum of squared differences between the\n",
    "observed and predicted values, but when there is high correlation between variables, LR model may be moderately or highly correlated with another.\n",
    "Multicollinearity exists when 2 or more predictors in regression model are correlated with another one. \n",
    "\n",
    "**Ridge Regression** use *L2 penalty*, that penalize the large coefficients to prevent overfitting.\n",
    "\n",
    "The tuning parameter Î» serves to control the relative impact, when Î» = 0 than penalty has no effect. As Î» grows to infitite the penalty grows which lead to shrinking coeffiecients to zero. Cross-Validation is used for selecting a good value for Î» as it's very important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e22b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In this cell we perform Ridge Regression for multiple alpha values that were \n",
    "generated in the cell before and calculate the coefficients, MSE, Bias and Variance for each model.\n",
    "Data are then used for plots.\n",
    "\"\"\"\n",
    "\n",
    "ridge = Ridge() \n",
    "coefs = [] \n",
    "\n",
    "mse = []\n",
    "squared_bias = []\n",
    "variance = []\n",
    " \n",
    "for a in alphas: \n",
    "    ridge.set_params(alpha = a) \n",
    "    ridge.fit(X, y) \n",
    "    coefs.append(ridge.coef_) \n",
    "    y_pred = ridge.predict(X_test)\n",
    "\n",
    "     # Calculate MSE\n",
    "    mse.append(np.mean((y_pred - y_test) ** 2))\n",
    "    \n",
    "    # Calculate bias and variance\n",
    "    bias = np.mean(y_pred - y_test)\n",
    "    squared_bias.append(bias ** 2)\n",
    "    variance.append(np.var(y_pred))\n",
    "     \n",
    "np.shape(coefs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f067522",
   "metadata": {},
   "source": [
    "### Plot description\n",
    "#### Plot axis\n",
    "This snippet of code visualizes how the coefficients (weights) of a Ridge Regression model change across different values of the regularization parameter, alpha. \n",
    "- X-axis: \n",
    "    - This is the regularization parameter for Ridge Regression.\n",
    "    - Smaller alpha values mean less regularization \n",
    "    - Larger alpha values mean more regularization, which shrinks the coefficients towards zero.\n",
    "- Y-axis\n",
    "    - Each line corresponds to the coefficient value (weight) of a feature in the model.\n",
    "\n",
    "### Plot behaviour\n",
    "- Small alpha values\n",
    "    - Coefficients are large (the model tries to fit the training data closely).\n",
    "- As alpha increases\n",
    "    - Coefficients shrink towards zero, indicating regularization\n",
    "    - If coefficient shrink to exactly 0 that mean feature is ignored in model\n",
    "- Very high alpha\n",
    "    - All coefficients tend towards zero, and the model behaves like a simple mean-based model \n",
    "\n",
    "### Key sights\n",
    "- Lower alpha indicate more complex model but potentially overfitting\n",
    "- Higher alpha stabilize coefficients or shrink to zero, indicating simpler/more stable model but may underfit data\n",
    "- If coefficients drop to zero early, that suggest that feature **may not cotribute to much** in model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b5e8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.gca() \n",
    "ax.plot(alphas, coefs) \n",
    "ax.set_xscale('log') \n",
    "plt.axis('tight') \n",
    "plt.xlabel('alpha') \n",
    "plt.ylabel('weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5d5139",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(alphas, squared_bias, color='black', label='Squared Bias', linewidth=2)\n",
    "plt.plot(alphas, variance, color='green', label='Variance', linewidth=2)\n",
    "plt.plot(alphas, mse, color='purple', label='Test MSE', linewidth=2)\n",
    "plt.xscale('log')\n",
    "plt.axhline(y=min(mse), color='red', linestyle='--', label='Minimum MSE')\n",
    "plt.scatter(alphas[np.argmin(mse)], min(mse), color='purple', marker='x', s=100, label='Minimum MSE Point')\n",
    "plt.title('Ridge Regression: Bias, Variance, and MSE vs. Regularization Parameter (Î»)')\n",
    "plt.xlabel('Regularization Parameter (Î»)')\n",
    "plt.ylabel('Error')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632ab9f7",
   "metadata": {},
   "source": [
    "Instead of arbitrarily choosing alpha's, we use cross-validation to choose the tuning parameter alpha. We can do this using the cross-\n",
    "validated ridge regression function, RidgeCV(). By default, the function performs generalized cross-validation (an efficient form of\n",
    "LOOCV), though this can be changed using the argument cv. Once the RidgeCV finished, we will display the alpha value and use it in Ridge function to get the MSE, Score and see the coefficients values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9721bc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV, Ridge\n",
    "from sklearn.metrics import r2_score\n",
    " \n",
    "# Use the Cross Validation to calculate the best alpha\n",
    "ridgecv = RidgeCV(alphas = alphas, scoring = 'neg_mean_squared_error') \n",
    "ridgecv.fit(X_train, y_train)\n",
    "ridgecv.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abee3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X_train and X_test are DataFrames\n",
    "ridge4 = Ridge(alpha=ridgecv.alpha_) \n",
    "ridge4.fit(X_train, y_train)\n",
    "y_pred = ridge4.predict(X_test)\n",
    "\n",
    "# Calculate and print MSE and RÂ² Score\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse:.5f}\")\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"RÂ² Score: {r2}\")\n",
    "coefficients = ridge4.coef_\n",
    "\n",
    "coef_series = pd.Series(ridge4.coef_, index=X.columns)\n",
    "sorted_coef = coef_series.reindex(coef_series.abs().sort_values(ascending=False).index)\n",
    "\n",
    "print(\"Coefficients sorted by absolute value:\")\n",
    "print(sorted_coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddaa30b",
   "metadata": {},
   "source": [
    "Lasso (Least Absolute Shrinkage and Selection Operator) is a type of linear regression that uses L1 regularization to prevent overfitting by penalizing the absolute size of regression coefficients.\n",
    "- It adds a penalty equivalent to the sum of the absolute values of coefficients to the loss function\n",
    "- Lasso shrinks some coefficients to exactly zero, effectively selecting a subset of features. This makes it a good choice when dealing with high-dimensional data or when some features are irrelevant\n",
    "- By shrinking coefficients, Lasso reduces variance at the cost of introducing some bias, helping to improve generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41328ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso, LassoCV \n",
    "from sklearn.preprocessing import scale  \n",
    " \n",
    "lasso = Lasso(max_iter = 10000) \n",
    "coefs = [] \n",
    "\n",
    "\n",
    "mse = []\n",
    "squared_bias = []\n",
    "variance = []\n",
    " \n",
    "for a in alphas: \n",
    "    lasso.set_params(alpha=a) \n",
    "    lasso.fit(scale(X_train), y_train) \n",
    "    coefs.append(lasso.coef_) \n",
    "\n",
    "    y_pred = lasso.predict(X_test)\n",
    "\n",
    "     # Calculate MSE\n",
    "    mse.append(np.mean((y_pred - y_test) ** 2))\n",
    "    \n",
    "    # Calculate bias and variance\n",
    "    bias = np.mean(y_pred - y_test)\n",
    "    squared_bias.append(bias ** 2)\n",
    "    variance.append(np.var(y_pred))\n",
    "     \n",
    "np.shape(coefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1bb9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(alphas, squared_bias, color='black', label='Squared Bias', linewidth=2)\n",
    "plt.plot(alphas, variance, color='green', label='Variance', linewidth=2)\n",
    "plt.plot(alphas, mse, color='purple', label='Test MSE', linewidth=2)\n",
    "plt.xscale('log')\n",
    "plt.axhline(y=min(mse), color='red', linestyle='--', label='Minimum MSE')\n",
    "plt.scatter(alphas[np.argmin(mse)], min(mse), color='purple', marker='x', s=100, label='Minimum MSE Point')\n",
    "plt.title('Ridge Regression: Bias, Variance, and MSE vs. Regularization Parameter (Î»)')\n",
    "plt.xlabel('Regularization Parameter (Î»)')\n",
    "plt.ylabel('Error')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc66b4e2",
   "metadata": {},
   "source": [
    "In this section we are using the Cross Validation for selecting the best alpha value as it's critical part of the Regression. Once we have the alpha value we Train with Lasso using best alpha, predictions are made. After that we calculate the metrics as MSE and Score, and show the relevant results from model to see coefficients of different columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56f95ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso, LassoCV\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "X_train = pd.DataFrame(X_train)\n",
    "X_test = pd.DataFrame(X_test)\n",
    "\n",
    "lasso_cv = LassoCV(alphas=alphas, cv=5, random_state=42).fit(X_train, y_train)\n",
    "\n",
    "best_alpha = lasso_cv.alpha_\n",
    "print(f\"Best Alpha: {best_alpha}\")\n",
    "\n",
    "lasso = Lasso(alpha=best_alpha)\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lasso.predict(X_test)\n",
    "\n",
    "mse = root_mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R2 Score: {r2}\")\n",
    "\n",
    "coefficients = pd.Series(lasso.coef_, index=X.columns)\n",
    "print(\"\\nCoefficients (Feature Name: Value):\")\n",
    "print(coefficients)\n",
    "\n",
    "features_to_drop = coefficients[coefficients == 0]\n",
    "\n",
    "print(\"\\nFeatures to Drop (Feature Name: Coefficient):\")\n",
    "for feature, coef in features_to_drop.items():\n",
    "    print(f\"{feature}: {coef}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
