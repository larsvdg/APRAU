{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1648,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold, LeaveOneOut, cross_val_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.utils import resample\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "df1 = pd.read_csv('Data_Class_1.csv')\n",
    "df2 = pd.read_csv('Data_Class_2.csv')\n",
    "df3 = pd.read_csv('Data_Class_3.csv')\n",
    "\n",
    "df = pd.concat([df1, df2, df3], axis=0)\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate descriptive statistics for the dataset\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_info = pd.DataFrame({\n",
    "    'Data Type': df.dtypes,\n",
    "    'Missing Values': df.isnull().sum(),\n",
    "    'Unique Values': df.nunique()\n",
    "})\n",
    "\n",
    "data_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot histograms for all numerical columns, to better understand their distributions.\n",
    "\n",
    "Here we can see distribution of various numerical variables using histograms. Each plot visualizes how the data is spread for features such as Altitude, Slope Orientation, Slope, and more. The density curves (where applicable) help indicate the shape of these distributions. For instance, Altitude and Slope Orientation exhibit fairly normal distributions, while variables like Vertical Distance to Water show skewed distributions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# List of numerical columns to plot\n",
    "numerical_columns = [\n",
    "    'Altitude', 'Slope_Orientation', 'Slope', \n",
    "    'Horizontal_Distance_To_Water', 'Vertical_Distance_To_Water', \n",
    "    'Horizontal_Distance_To_Roadways', 'Shadow_Index_9h', \n",
    "    'Shadow_Index_12h', 'Shadow_Index_15h', \n",
    "    'Horizontal_Distance_To_Fire_Points', 'Canopy_Density', \n",
    "    'Rainfall_Summer', 'Rainfall_Winter', 'Wind_Exposure_Level'\n",
    "]\n",
    "\n",
    "# Set up the subplots, adjusting number of rows and columns to fit all features\n",
    "num_plots = len(numerical_columns)\n",
    "cols = 3\n",
    "rows = num_plots // cols + (num_plots % cols > 0)\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(18, 12))\n",
    "fig.suptitle('Distribution of Numerical Variables', fontsize=16)\n",
    "\n",
    "# Plot histograms for each numerical feature\n",
    "for i, col in enumerate(numerical_columns):\n",
    "    row = i // cols\n",
    "    col_idx = i % cols\n",
    "    sns.histplot(df[col], kde=True, bins=20, ax=axes[row, col_idx])\n",
    "    axes[row, col_idx].set_title(f'{col} Distribution')\n",
    "\n",
    "# Hide any unused subplots\n",
    "for i in range(num_plots, rows * cols):\n",
    "    fig.delaxes(axes.flat[i])\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphs for bivariate analysis, to see scatter plots between the numerical variables and target variable to observe any trends or patters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the style for the visualizations\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Plot bar plots for categorical variables\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "fig.suptitle('Distribution of Categorical Variables')\n",
    "\n",
    "# Plot for Soil_Type\n",
    "sns.countplot(data=df, x='Soil_Type', hue='Soil_Type', ax=axes[0], palette='viridis', legend=False)\n",
    "axes[0].set_title('Soil Type Distribution')\n",
    "axes[0].tick_params(axis='x', rotation=90)  \n",
    "\n",
    "# Plot for Wilderness_Area\n",
    "sns.countplot(data=df, x='Wilderness_Area', hue='Wilderness_Area', ax=axes[1], palette='coolwarm', legend=False)\n",
    "axes[1].set_title('Wilderness Area Distribution')\n",
    "\n",
    "# Plot for Vegetation_Type\n",
    "sns.countplot(data=df, x='Vegetation_Type', hue='Vegetation_Type', ax=axes[2], palette='Set2', legend=False)\n",
    "axes[2].set_title('Vegetation Type Distribution')\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Observations:\n",
    "Altitude: The vegetation types appear to be well-separated by altitude. Type_1 is at a higher altitude, while Type_3 is at a lower altitude, with Type_2 in between. This suggests altitude may be a strong feature for distinguishing between the types.\n",
    "\n",
    "Slope_Orientation: All three types show overlap in terms of slope orientation, so it doesn't seem to differentiate vegetation types strongly.\n",
    "\n",
    "Slope: There is no significant distinction between the vegetation types based on slope alone, as all seem to occupy similar ranges.\n",
    "\n",
    "Horizontal and Vertical Distance to Water: These variables show some degree of separation, especially for Type_3, which tends to have smaller horizontal distances to water. Type_1 and Type_2 overlap more but still show some separation.\n",
    "\n",
    "Shadow Index (9h, 12h, 15h): There’s a fair amount of overlap in the shadow indices among the vegetation types, meaning these variables may not be significant in distinguishing between them.\n",
    "\n",
    "Horizontal Distance to Roadways: This feature appears to be quite distinct, especially for Type_1, which has a wider range and larger distances from roadways compared to Type_2 and Type_3.\n",
    "\n",
    "Horizontal Distance to Fire Points: This variable has some separation between vegetation types, with Type_1 having much higher distances to fire points than Type_2 and Type_3, which cluster lower on this axis.\n",
    "\n",
    "Canopy Density: All vegetation types appear to have similar canopy densities, making it difficult to differentiate between them based on this feature.\n",
    "\n",
    "Rainfall (Summer and Winter): The rainfall in both seasons seems to be very similar across vegetation types, showing little to no variation or overlap.\n",
    "\n",
    "Wind Exposure Level: There is minimal distinction among the vegetation types based on wind exposure, as all appear to have similar values.\n",
    "\n",
    "## Key Insights:\n",
    "Altitude and horizontal distance to roadways/fire points appear to be strong variables for separating vegetation types, particularly Type_1.\n",
    "Some features, like slope orientation, shadow indices, and canopy density, show a lot of overlap, suggesting they might not be as important in classification.\n",
    "Horizontal distance to water also provides some separability for Type_3, which might help in classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the style for the visualizations\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# List of numerical variables to plot against Vegetation_Type\n",
    "numerical_columns = [\n",
    "    'Altitude', 'Slope_Orientation', 'Slope', \n",
    "    'Horizontal_Distance_To_Water', 'Vertical_Distance_To_Water', \n",
    "    'Horizontal_Distance_To_Roadways', 'Shadow_Index_9h', \n",
    "    'Shadow_Index_12h', 'Shadow_Index_15h', \n",
    "    'Horizontal_Distance_To_Fire_Points', 'Canopy_Density', \n",
    "    'Rainfall_Summer', 'Rainfall_Winter', 'Wind_Exposure_Level'\n",
    "]\n",
    "\n",
    "# Set up the subplots grid\n",
    "num_plots = len(numerical_columns)\n",
    "cols = 3  # Number of columns\n",
    "rows = num_plots // cols + (num_plots % cols > 0)  # Number of rows\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(18, 4 * rows))\n",
    "fig.suptitle('Scatter Plots of Numerical Variables vs Vegetation Type', fontsize=16)\n",
    "\n",
    "# Plot each numerical variable vs Vegetation_Type\n",
    "for i, col in enumerate(numerical_columns):\n",
    "    row = i // cols\n",
    "    col_idx = i % cols\n",
    "    # Use stripplot for jitter effect or scatterplot directly\n",
    "    sns.stripplot(x='Vegetation_Type', y=col, data=df, ax=axes[row, col_idx], jitter=True, palette='Set2', hue='Vegetation_Type', alpha=0.6, legend=False)\n",
    "    axes[row, col_idx].set_title(f'{col} vs Vegetation Type')\n",
    "    axes[row, col_idx].tick_params(axis='x', rotation=45)  # Rotate x labels for readability\n",
    "\n",
    "# Hide any empty subplots (if any)\n",
    "for i in range(num_plots, rows * cols):\n",
    "    fig.delaxes(axes.flat[i])\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Convert Vegetation_Type to numerical labels using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "df['Vegetation_Type_Encoded'] = label_encoder.fit_transform(df['Vegetation_Type'])\n",
    "\n",
    "# List of numerical columns to include in the correlation heatmap\n",
    "numerical_columns = [\n",
    "    'Altitude', 'Slope_Orientation', 'Slope', \n",
    "    'Horizontal_Distance_To_Water', 'Vertical_Distance_To_Water', \n",
    "    'Horizontal_Distance_To_Roadways', 'Shadow_Index_9h', \n",
    "    'Shadow_Index_12h', 'Shadow_Index_15h', \n",
    "    'Horizontal_Distance_To_Fire_Points', 'Canopy_Density', \n",
    "    'Rainfall_Summer', 'Rainfall_Winter', 'Wind_Exposure_Level',\n",
    "    'Vegetation_Type_Encoded'  # Include encoded Vegetation_Type\n",
    "]\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr_matrix = df[numerical_columns].corr()\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Correlation Heatmap of Numerical Variables')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Box plots for comparing categorical variables.\n",
    "\n",
    "In the graphs, we observe the relationship between Soil Type and Vegetation Type. The first two vegetation types (Type 1 and Type 2) show similar distributions for the various soil types. However, Vegetation Type 3 exhibits a distinct soil type distribution, indicating that it occurs in areas with different soil characteristics.\n",
    "\n",
    "For the Wilderness Area vs Vegetation Type plot, we see that Vegetation Types 1 and 2 share similar wilderness areas (Areas 1, 2, and 3). In contrast, Vegetation Type 3 appears to be predominantly associated with Area 4, suggesting a different wilderness distribution compared to the other vegetation types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the style for the visualizations\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Plot box plots for categorical variables vs Vegetation_Type\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "fig.suptitle('Box Plots of Categorical Variables vs Vegetation Type', fontsize=16)\n",
    "\n",
    "# Box plot for Soil_Type vs Vegetation_Type\n",
    "sns.boxplot(x='Vegetation_Type', y='Soil_Type', data=df, ax=axes[0], palette='Set2', hue='Vegetation_Type', legend=False)\n",
    "axes[0].set_title('Soil Type vs Vegetation Type')\n",
    "axes[0].set_xlabel('Vegetation Type')\n",
    "axes[0].set_ylabel('Soil Type')\n",
    "\n",
    "# Box plot for Wilderness_Area vs Vegetation_Type\n",
    "sns.boxplot(x='Vegetation_Type', y='Wilderness_Area', data=df, ax=axes[1], palette='Set2', hue='Vegetation_Type', legend=False)\n",
    "axes[1].set_title('Wilderness Area vs Vegetation Type')\n",
    "axes[1].set_xlabel('Vegetation Type')\n",
    "axes[1].set_ylabel('Wilderness Area')\n",
    "\n",
    "# Remove automatic legends\n",
    "axes[0].legend([], [], title='Vegetation Type', loc='upper right')\n",
    "axes[1].legend([], [], title='Vegetation Type', loc='upper right')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tryout LR all variables\n",
    "The model demonstrates good predictive ability with an accuracy of up to 84.6%. However, performance is weaker for Type_2 vegetation, which suggests room for improvement in capturing this class. Further efforts can be made to balance the model's performance across all vegetation types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 2: Load your dataset (adjust the file path as needed)\n",
    "# Assuming df is already loaded with the Vegetation_Type and Vegetation_Type_Encoded\n",
    "\n",
    "# Step 3: Prepare the data\n",
    "# Define numerical features\n",
    "numerical_features = [ \n",
    "    'Altitude', 'Slope_Orientation', 'Slope', \n",
    "    'Horizontal_Distance_To_Water', 'Vertical_Distance_To_Water', \n",
    "    'Horizontal_Distance_To_Roadways', 'Shadow_Index_9h', \n",
    "    'Shadow_Index_12h', 'Shadow_Index_15h', \n",
    "    'Horizontal_Distance_To_Fire_Points', 'Canopy_Density'\n",
    "    ,'Rainfall_Summer', 'Rainfall_Winter', 'Wind_Exposure_Level'\n",
    "]\n",
    "\n",
    "# Define categorical features\n",
    "categorical_features = ['Soil_Type', 'Wilderness_Area']  # Add your categorical columns here\n",
    "\n",
    "# Separate target variable (Vegetation_Type) and features\n",
    "X = df.drop(['Vegetation_Type', \"Vegetation_Type_Encoded\"], axis=1)\n",
    "y = df['Vegetation_Type']\n",
    "\n",
    "# Convert categorical variables into dummy variables\n",
    "X = pd.get_dummies(X, columns=categorical_features, drop_first=True)\n",
    "\n",
    "# Step 4: Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 5: Feature Scaling (Standardize the data)\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Step 6: Apply Logistic Regression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Step 7: Make predictions on the test data\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# Step 8: Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n",
    "\n",
    "# Step 9: Visualize the confusion matrix\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual Values')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 2: Load your dataset (adjust the file path as needed)\n",
    "# Assuming df is already loaded with the Vegetation_Type and Vegetation_Type_Encoded\n",
    "\n",
    "# Step 3: Prepare the data\n",
    "# Separate target variable (Vegetation_Type) and features\n",
    "y = df['Vegetation_Type']\n",
    "\n",
    "# Define numerical and categorical features\n",
    "numerical_features = ['Altitude', 'Slope_Orientation', 'Horizontal_Distance_To_Roadways',\n",
    "                      'Shadow_Index_9h', 'Shadow_Index_12h', \n",
    "                      \n",
    "                      #, 'Shadow_Index_15h',\n",
    "                      #'Rainfall_Summer', 'Rainfall_Winter', 'Wind_Exposure_Level'\n",
    "                      ]\n",
    "\n",
    "categorical_features = ['Soil_Type', 'Wilderness_Area']  # Include your categorical columns\n",
    "\n",
    "# Convert categorical variables into dummy variables\n",
    "X_categorical = pd.get_dummies(df[categorical_features], drop_first=True)\n",
    "\n",
    "# Keep numerical variables as is\n",
    "X_numerical = df[numerical_features]\n",
    "\n",
    "# Combine numerical and dummy categorical variables\n",
    "X = pd.concat([X_numerical, X_categorical], axis=1)\n",
    "\n",
    "# Step 4: Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "# Step 5: Feature Scaling (Standardize the data)\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Step 6: Apply Logistic Regression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Step 7: Make predictions on the test data\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# Step 8: Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n",
    "\n",
    "# Step 9: Visualize the confusion matrix\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual Values')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y = df['Vegetation_Type']\n",
    "\n",
    "# Define numerical and categorical features\n",
    "numerical_features = ['Altitude', 'Slope_Orientation', 'Horizontal_Distance_To_Roadways',\n",
    "                      'Shadow_Index_9h', 'Shadow_Index_12h']\n",
    "\n",
    "categorical_features = ['Soil_Type', 'Wilderness_Area']  # Include your categorical columns\n",
    "\n",
    "# Step 3: Convert categorical variables into dummy variables\n",
    "# It's important to include only the selected features after converting to dummy\n",
    "X_categorical = pd.get_dummies(df[categorical_features], drop_first=True)\n",
    "\n",
    "# Keep numerical variables as is\n",
    "X_numerical = df[numerical_features]\n",
    "\n",
    "# Combine numerical and dummy categorical variables\n",
    "X = pd.concat([X_numerical, X_categorical], axis=1)\n",
    "\n",
    "# Step 4: Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 5: Feature Scaling (Standardize the data)\n",
    "scaler = MinMaxScaler()  # Create a StandardScaler object\n",
    "X_train = scaler.fit_transform(X_train)  # Fit and transform the training data\n",
    "X_test = scaler.transform(X_test)  # Only transform the test data\n",
    "\n",
    "# Step 6: Apply Linear Discriminant Analysis\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_train, y_train)\n",
    "\n",
    "# Step 7: Make predictions on the test data\n",
    "y_pred = lda.predict(X_test)\n",
    "\n",
    "# Step 8: Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n",
    "\n",
    "# Step 9: Visualize the confusion matrix\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual Values')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis  # Import QDA instead of LDA\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming df is already defined and loaded with your dataset\n",
    "y = df['Vegetation_Type']\n",
    "\n",
    "# Define numerical and categorical features\n",
    "numerical_features = ['Altitude', 'Slope_Orientation', 'Horizontal_Distance_To_Roadways',\n",
    "                      'Shadow_Index_9h', 'Shadow_Index_12h']\n",
    "\n",
    "categorical_features = ['Soil_Type', 'Wilderness_Area']  # Include your categorical columns\n",
    "\n",
    "# Step 3: Convert categorical variables into dummy variables\n",
    "# It's important to include only the selected features after converting to dummy\n",
    "X_categorical = pd.get_dummies(df[categorical_features], drop_first=True)\n",
    "\n",
    "# Keep numerical variables as is\n",
    "X_numerical = df[numerical_features]\n",
    "\n",
    "# Combine numerical and dummy categorical variables\n",
    "X = pd.concat([X_numerical, X_categorical], axis=1)\n",
    "\n",
    "# Step 4: Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 5: Feature Scaling (Standardize the data)\n",
    "scaler = MinMaxScaler()  # Create a MinMaxScaler object\n",
    "X_train = scaler.fit_transform(X_train)  # Fit and transform the training data\n",
    "X_test = scaler.transform(X_test)  # Only transform the test data\n",
    "\n",
    "# Step 6: Apply Quadratic Discriminant Analysis\n",
    "qda = QuadraticDiscriminantAnalysis()  # Create a QDA object\n",
    "qda.fit(X_train, y_train)  # Fit the QDA model to the training data\n",
    "\n",
    "# Step 7: Make predictions on the test data\n",
    "y_pred = qda.predict(X_test)\n",
    "\n",
    "# Step 8: Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n",
    "\n",
    "# Step 9: Visualize the confusion matrix\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual Values')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data for models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1661,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df, target, numerical_features, categorical_features):\n",
    "    # Combine numerical and categorical features\n",
    "    selected_features = numerical_features + categorical_features\n",
    "    \n",
    "    # Separate features and target variable\n",
    "    X = df[selected_features]\n",
    "    y = df[target]\n",
    "    \n",
    "    # Convert categorical variables into dummy variables\n",
    "    X = pd.get_dummies(X, drop_first=True)\n",
    "    \n",
    "    # Scale numerical features\n",
    "    scaler = MinMaxScaler()\n",
    "    X[numerical_features] = scaler.fit_transform(X[numerical_features])\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Holdout validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1662,
   "metadata": {},
   "outputs": [],
   "source": [
    "def holdout_validation(model, X, y, test_size=0.2):\n",
    "    # Train-Test Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "    \n",
    "    # Fit the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "    print(f\"Holdout Validation Accuracy: {accuracy}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "    print(\"Classification Report:\")\n",
    "    print(class_report)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    # sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "    # plt.title('Confusion Matrix (Holdout Validation)')\n",
    "    # plt.ylabel('Actual Values')\n",
    "    # plt.xlabel('Predicted Values')\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1663,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cross_validation(model, X, y, k=5):\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(model, X, y, cv=kf)\n",
    "    print(f\"K-Fold Cross Validation (k={k}) Accuracy Scores: {scores}\")\n",
    "    print(f\"Mean Accuracy: {np.mean(scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOOCV Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1664,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loocv_cross_validation(model, X, y, max_samples=None):\n",
    "    loo = LeaveOneOut()\n",
    "    \n",
    "    # Use max_samples to limit the number of LOOCV evaluations\n",
    "    if max_samples is not None and max_samples < len(X):\n",
    "        indices = np.random.choice(len(X), max_samples, replace=False)\n",
    "        X = X.iloc[indices]\n",
    "        y = y.iloc[indices]\n",
    "\n",
    "    accuracies = []\n",
    "    \n",
    "    for train_index, test_index in loo.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "    print(f\"LOOCV Accuracy: {np.mean(accuracies):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1665,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_evaluation(model, X, y, n_iterations=1000):\n",
    "    n_size = int(len(X) * 0.9)  # Use 90% of the data for bootstrap sample\n",
    "    scores = []\n",
    "\n",
    "    for i in range(n_iterations):\n",
    "        # Generate a bootstrap sample\n",
    "        X_train, y_train = resample(X, y, n_samples=n_size, random_state=i)\n",
    "        \n",
    "        # Fit the model on the bootstrap sample\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Test on the out-of-bag data (remaining 10%)\n",
    "        X_test, y_test = X.drop(X_train.index), y.drop(y_train.index)\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        scores.append(accuracy)\n",
    "\n",
    "    print(f\"Bootstrap Mean Accuracy: {np.mean(scores)}\")\n",
    "    print(f\"Bootstrap Standard Deviation: {np.std(scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline for running resempling in models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1666,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cross_validation_pipeline(df, model, target='Vegetation_Type', use_loocv=False, use_bootstrap=False):\n",
    "    # Define numerical and categorical features\n",
    "    numerical_features = ['Altitude', 'Slope_Orientation', 'Horizontal_Distance_To_Roadways', \n",
    "                          'Shadow_Index_9h', 'Shadow_Index_12h']\n",
    "    categorical_features = ['Soil_Type', 'Wilderness_Area']\n",
    "    \n",
    "    # Step 1: Preprocess the data\n",
    "    X, y = preprocess_data(df, target, numerical_features, categorical_features)\n",
    "\n",
    "    \n",
    "    # Step 2: Perform Holdout Validation\n",
    "    print(\"Holdout Validation Results:\")\n",
    "    holdout_validation(model, X, y, test_size=0.2)\n",
    "    \n",
    "    # Step 3: Perform K-Fold Cross Validation\n",
    "    print(\"\\nK-Fold Cross Validation Results (k=5):\")\n",
    "    k_fold_cross_validation(model, X, y, k=5)\n",
    "    \n",
    "    print(\"\\nK-Fold Cross Validation Results (k=10):\")\n",
    "    k_fold_cross_validation(model, X, y, k=10)\n",
    "    \n",
    "    # Step 4: Optionally Perform Leave-One-Out Cross Validation (LOOCV)\n",
    "    if use_loocv:\n",
    "        print(\"\\nLOOCV Results:\")\n",
    "        loocv_cross_validation(model, X, y, max_samples=1000)\n",
    "    \n",
    "    # Step 5: Optionally Perform Bootstrap Evaluation\n",
    "    if use_bootstrap:\n",
    "        print(\"\\nBootstrap Evaluation Results:\")\n",
    "        bootstrap_evaluation(model, X, y, n_iterations=1000)  # Limit Bootstrap iterations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying the types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_model = LogisticRegression()\n",
    "lda_model = LinearDiscriminantAnalysis()\n",
    "qda_model = QuadraticDiscriminantAnalysis()\n",
    "# Example usage without LOOCV and Bootstrap (default)\n",
    "#run_cross_validation_pipeline(df, logreg_model)\n",
    "\n",
    "# Example usage with LOOCV and Bootstrap (optional)\n",
    "run_cross_validation_pipeline(df, logreg_model, use_loocv=True, use_bootstrap=True)\n",
    "\n",
    "# run_cross_validation_pipeline(df, lda_model, use_loocv=True, use_bootstrap=True)\n",
    "\n",
    "# run_cross_validation_pipeline(df, qda_model, use_loocv=True, use_bootstrap=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def compare_models(models, X_train, X_test, y_train, y_test):\n",
    "    comparison_results = []\n",
    "    \n",
    "    for name, model in models:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "        comparison_results.append({'Model': name, 'Accuracy': accuracy, 'F1 Score': f1})\n",
    "\n",
    "    return pd.DataFrame(comparison_results)\n",
    "\n",
    "# Example usage\n",
    "models = [('Logistic Regression', logreg_model), ('LDA', lda_model), ('QDA', qda_model)]\n",
    "compare_models(models, X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "There are two major problems related to training models: **overfitting** and **underfitting**:\n",
    "\n",
    "- Overfitting: The model performs well on the training set but not so well on unseen (test) data.\n",
    "- Underfitting: Neither performs well on the train set nor on the test set.\n",
    "\n",
    "**Regularization** is implemented to avoid overfitting of the data, especially when there is a large variance between train and test set\n",
    "performances. There are different methods of reducing the model complexity and preventing overfitting in linear models which are *Ridge* and *Lasso*\n",
    "*Regression Models*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data for Ridge Regression\n",
    "\n",
    "Before applying Ridge Regression, we need to properly prepare the dataset. This involves several steps, such as:\n",
    "\n",
    "1. **Feature Selection/Engineering**: Ensure relevant features are selected and encoded.\n",
    "2. **Splitting the Data**: Divide the dataset into training and test sets.\n",
    "3. **Feature Scaling**: Apply scaling to ensure features are on a similar scale, as Ridge Regression is sensitive to feature magnitudes.\n",
    "\n",
    "Below is an example of how to prepare data for Ridge Regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1669,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all libraries \n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet \n",
    "from sklearn.metrics import mean_squared_error , root_mean_squared_error\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler \n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "df.head(5)\n",
    "\n",
    "def prepare_dataset(df, target, categorical_features, numerical_features, numberical_target_var = True):\n",
    "\n",
    "    df.isnull().sum()*100/df.shape[0]\n",
    "    df.head()\n",
    "    # Encode the target variable if it's categorical\n",
    "    if numberical_target_var == False:\n",
    "        le = LabelEncoder()\n",
    "        y = le.fit_transform(df[target])\n",
    "    \n",
    "    # Convert the categorical columns into the boolean type, remove the Id col\n",
    "    dummies = pd.get_dummies(df[categorical_features])\n",
    "\n",
    "    # Drop the categorical features from the X\n",
    "    X_ = df.drop(categorical_features, axis = 1) \n",
    "    # Drop the target variable\n",
    "    X_ = X_.drop(target, axis = 1) \n",
    "    X_ = X_.drop([\"Id\"], axis=1) # TODO remove\n",
    "\n",
    "    # Convert all numerical values to the float type\n",
    "    X = X_.apply(lambda col: col.astype('float64') if col.dtype in ['int64', 'float32', 'float64'] else col)\n",
    "    # Merge the dummies (categorical var w numerical variables)\n",
    "    X = pd.concat([X, dummies], axis = 1) \n",
    "\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the head of the dataset before any modification\n",
    "df = df.drop(\"Vegetation_Type_Encoded\", axis=1)\n",
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the column with the independent variable (Salary), and columns for which we created dummy variables \n",
    "y = df.Vegetation_Type\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "numerical_features = [\"Altitude\", \"Slope_Orientation\", \"Slope\", \"Horizontal_Distance_To_Water\"\n",
    "    ,\"Vertical_Distance_To_Water\", \"Horizontal_Distance_To_Roadways\", \"Shadow_Index_9h\",\n",
    "    \"Shadow_Index_12h\", \"Shadow_Index_15h\", \"Horizontal_Distance_To_Fire_Points\", \n",
    "    \"Canopy_Density\", \"Rainfall_Summer\", \"Rainfall_Winter\", \"Wind_Exposure_Level\"]\n",
    "categorical_features = [\"Wilderness_Area\", \"Soil_Type\"]\n",
    "target_variable = \"Vegetation_Type\"\n",
    "\n",
    "# Get x,y data\n",
    "X, y = prepare_dataset(df, target_variable, categorical_features, numerical_features, False)\n",
    "# Show info about X data\n",
    "X.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show head of the X data\n",
    "X.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1673,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "sc=StandardScaler() \n",
    "X_train=sc.fit_transform(X_train) \n",
    "X_test=sc.transform(X_test)\n",
    "\n",
    "#from sklearn.preprocessing import minmax_scale \n",
    "# minmax_scale = MinMaxScaler()\n",
    "# X_train=minmax_scale(X_train, axis=0) \n",
    "# X_test=sc.transform(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1674,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate an NumPy array of alphas from small to large \n",
    "# number which are logarithmically spaced, meaning they decrease exponentially\n",
    "\n",
    "alphas = 10**np.linspace(10,-2,100)*0.5 \n",
    "\n",
    "# Uncomment the alphas if you want to see the array\n",
    "# alphas "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Resgression\n",
    "Is a variation of linear regression, specifically designed to address multicollinearity in the dataset. In linear regression, the goal is to find the best-fitting hyperplane that minimizes the sum of squared differences between the\n",
    "observed and predicted values, but when there is high correlation between variables, LR model may be moderately or highly correlated with another.\n",
    "Multicollinearity exists when 2 or more predictors in regression model are correlated with another one. \n",
    "\n",
    "**Ridge Regression** use *L2 penalty*, that penalize the large coefficients to prevent overfitting.\n",
    "\n",
    "The tuning parameter λ serves to control the relative impact, when λ = 0 than penalty has no effect. As λ grows to infitite the penalty grows which lead to shrinking coeffiecients to zero. Cross-Validation is used for selecting a good value for λ as it's very important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model \n",
    " \n",
    "ridge = Ridge() \n",
    "coefs = [] \n",
    "\n",
    "mse = []\n",
    "squared_bias = []\n",
    "variance = []\n",
    " \n",
    "for a in alphas: \n",
    "    ridge.set_params(alpha = a) \n",
    "    ridge.fit(X, y) \n",
    "    coefs.append(ridge.coef_) \n",
    "    y_pred = ridge.predict(X_test)\n",
    "\n",
    "     # Calculate MSE\n",
    "    mse.append(np.mean((y_pred - y_test) ** 2))\n",
    "    \n",
    "    # Calculate bias and variance\n",
    "    bias = np.mean(y_pred - y_test)\n",
    "    squared_bias.append(bias ** 2)\n",
    "    variance.append(np.var(y_pred))\n",
    "     \n",
    "np.shape(coefs)\n",
    "\n",
    "ax = plt.gca() \n",
    "ax.plot(alphas, coefs) \n",
    "ax.set_xscale('log') \n",
    "plt.axis('tight') \n",
    "plt.xlabel('alpha') \n",
    "plt.ylabel('weights')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(alphas, squared_bias, color='black', label='Squared Bias', linewidth=2)\n",
    "plt.plot(alphas, variance, color='green', label='Variance', linewidth=2)\n",
    "plt.plot(alphas, mse, color='purple', label='Test MSE', linewidth=2)\n",
    "plt.xscale('log')\n",
    "plt.axhline(y=min(mse), color='red', linestyle='--', label='Minimum MSE')\n",
    "plt.scatter(alphas[np.argmin(mse)], min(mse), color='purple', marker='x', s=100, label='Minimum MSE Point')\n",
    "plt.title('Ridge Regression: Bias, Variance, and MSE vs. Regularization Parameter (λ)')\n",
    "plt.xlabel('Regularization Parameter (λ)')\n",
    "plt.ylabel('Error')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV, Ridge\n",
    "from sklearn.metrics import r2_score\n",
    " \n",
    "X_train, X_test , y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=1) \n",
    "# Use the Cross Validation to calculate the best alpha\n",
    "ridgecv = RidgeCV(alphas = alphas, scoring = 'neg_mean_squared_error') \n",
    "ridgecv.fit(X_train, y_train)\n",
    "ridgecv.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X_train and X_test are DataFrames\n",
    "ridge4 = Ridge(alpha=ridgecv.alpha_) \n",
    "ridge4.fit(X_train, y_train)\n",
    "y_pred = ridge4.predict(X_test)\n",
    "\n",
    "# Calculate and print MSE and R² Score\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse:.5f}\")\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"R² Score: {r2}\")\n",
    "\n",
    "coefficients = ridge4.coef_\n",
    "\n",
    "coef_series = pd.Series(ridge4.coef_, index=X_train.columns)\n",
    "sorted_coef = coef_series.reindex(coef_series.abs().sort_values(ascending=False).index)\n",
    "\n",
    "# print(\"Coefficients sorted by absolute value:\")\n",
    "# print(sorted_coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso, LassoCV \n",
    "from sklearn.preprocessing import scale  \n",
    " \n",
    "lasso = Lasso(max_iter = 10000) \n",
    "coefs = [] \n",
    " \n",
    "for a in alphas: \n",
    "    lasso.set_params(alpha=a) \n",
    "    lasso.fit(scale(X_train), y_train) \n",
    "    coefs.append(lasso.coef_) \n",
    "     \n",
    "ax = plt.gca() \n",
    "ax.plot(alphas*2, coefs) \n",
    "ax.set_xscale('log') \n",
    "plt.axis('tight') \n",
    "plt.xlabel('alpha') \n",
    "plt.ylabel('weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso reggression\n",
    "\n",
    "This is example usecase of Lasso regression with values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso, LassoCV\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming X_train2 and y_train2 are your training data\n",
    "# Convert X_train2 to DataFrame if it is not already\n",
    "X_train = pd.DataFrame(X_train)\n",
    "X_test = pd.DataFrame(X_test)\n",
    "\n",
    "# Step 1: Use LassoCV to find the best alpha (regularization parameter)\n",
    "alphas = np.logspace(-5, 1, 100)  # Test alpha values from 10^-5 to 10^1\n",
    "lasso_cv = LassoCV(alphas=alphas, cv=5, random_state=42).fit(X_train, y_train)\n",
    "\n",
    "# Step 2: Get the best alpha value\n",
    "best_alpha = lasso_cv.alpha_\n",
    "print(f\"Best Alpha: {best_alpha}\")\n",
    "\n",
    "# Step 3: Train Lasso with the best alpha\n",
    "lasso = Lasso(alpha=best_alpha)\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "# Step 4: Make predictions\n",
    "y_pred = lasso.predict(X_test)\n",
    "\n",
    "# Step 5: Calculate metrics (MSE and R2 Score)\n",
    "mse = root_mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R2 Score: {r2}\")\n",
    "\n",
    "# Step 6: Identify the coefficients and which features to keep/drop\n",
    "coefficients = pd.Series(lasso.coef_, index=X_train.columns)\n",
    "print(\"\\nCoefficients (Feature Name: Value):\")\n",
    "print(coefficients)\n",
    "\n",
    "# Step 8: List features with zero coefficients (can be removed)\n",
    "features_to_drop = coefficients[coefficients == 0]\n",
    "\n",
    "# Format the output to show feature names instead of index numbers\n",
    "print(\"\\nFeatures to Drop (Feature Name: Coefficient):\")\n",
    "for feature, coef in features_to_drop.items():\n",
    "    print(f\"{feature}: {coef}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
