{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see the head of our joined dataset, showing a glimpse of how the data from all three classes is combined into a unified dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold, LeaveOneOut, cross_val_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.utils import resample\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "df1 = pd.read_csv('Data_Class_1.csv')\n",
    "df2 = pd.read_csv('Data_Class_2.csv')\n",
    "df3 = pd.read_csv('Data_Class_3.csv')\n",
    "\n",
    "df = pd.concat([df1, df2, df3], axis=0)\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see how many rows and columns it has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see descriptive statistics for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see data types, and also if there are missing values, or how many unique values is there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_info = pd.DataFrame({\n",
    "    'Data Type': df.dtypes,\n",
    "    'Missing Values': df.isnull().sum(),\n",
    "    'Unique Values': df.nunique()\n",
    "})\n",
    "\n",
    "data_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot histograms for all numerical columns, to better understand their distributions.\n",
    "\n",
    "Here we can see distribution of various numerical variables using histograms. Each plot visualizes how the data is spread for features such as Altitude, Slope Orientation, Slope, and more. The density curves (where applicable) help indicate the shape of these distributions. For instance, Altitude and Slope Orientation exhibit fairly normal distributions, while variables like Vertical Distance to Water show skewed distributions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# List of numerical columns to plot\n",
    "numerical_columns = [\n",
    "    'Altitude', 'Slope_Orientation', 'Slope', \n",
    "    'Horizontal_Distance_To_Water', 'Vertical_Distance_To_Water', \n",
    "    'Horizontal_Distance_To_Roadways', 'Shadow_Index_9h', \n",
    "    'Shadow_Index_12h', 'Shadow_Index_15h', \n",
    "    'Horizontal_Distance_To_Fire_Points', 'Canopy_Density', \n",
    "    'Rainfall_Summer', 'Rainfall_Winter', 'Wind_Exposure_Level'\n",
    "]\n",
    "\n",
    "# Set up the subplots, adjusting number of rows and columns to fit all features\n",
    "num_plots = len(numerical_columns)\n",
    "cols = 3\n",
    "rows = num_plots // cols + (num_plots % cols > 0)\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(18, 12))\n",
    "fig.suptitle('Distribution of Numerical Variables', fontsize=16)\n",
    "\n",
    "# Plot histograms for each numerical feature\n",
    "for i, col in enumerate(numerical_columns):\n",
    "    row = i // cols\n",
    "    col_idx = i % cols\n",
    "    sns.histplot(df[col], kde=True, bins=20, ax=axes[row, col_idx])\n",
    "    axes[row, col_idx].set_title(f'{col} Distribution')\n",
    "\n",
    "# Hide any unused subplots\n",
    "for i in range(num_plots, rows * cols):\n",
    "    fig.delaxes(axes.flat[i])\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphs for bivariate analysis, to see scatter plots between the numerical variables and target variable to observe any trends or patters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the style for the visualizations\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Plot bar plots for categorical variables\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "fig.suptitle('Distribution of Categorical Variables')\n",
    "\n",
    "# Plot for Soil_Type\n",
    "sns.countplot(data=df, x='Soil_Type', hue='Soil_Type', ax=axes[0], palette='viridis', legend=False)\n",
    "axes[0].set_title('Soil Type Distribution')\n",
    "axes[0].tick_params(axis='x', rotation=90)  \n",
    "\n",
    "# Plot for Wilderness_Area\n",
    "sns.countplot(data=df, x='Wilderness_Area', hue='Wilderness_Area', ax=axes[1], palette='coolwarm', legend=False)\n",
    "axes[1].set_title('Wilderness Area Distribution')\n",
    "\n",
    "# Plot for Vegetation_Type\n",
    "sns.countplot(data=df, x='Vegetation_Type', hue='Vegetation_Type', ax=axes[2], palette='Set2', legend=False)\n",
    "axes[2].set_title('Vegetation Type Distribution')\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Observations:\n",
    "Altitude: The vegetation types appear to be well-separated by altitude. Type_1 is at a higher altitude, while Type_3 is at a lower altitude, with Type_2 in between. This suggests altitude may be a strong feature for distinguishing between the types.\n",
    "\n",
    "Slope_Orientation: All three types show overlap in terms of slope orientation, so it doesn't seem to differentiate vegetation types strongly.\n",
    "\n",
    "Slope: There is no significant distinction between the vegetation types based on slope alone, as all seem to occupy similar ranges.\n",
    "\n",
    "Horizontal and Vertical Distance to Water: These variables show some degree of separation, especially for Type_3, which tends to have smaller horizontal distances to water. Type_1 and Type_2 overlap more but still show some separation.\n",
    "\n",
    "Shadow Index (9h, 12h, 15h): Thereâ€™s a fair amount of overlap in the shadow indices among the vegetation types, meaning these variables may not be significant in distinguishing between them.\n",
    "\n",
    "Horizontal Distance to Roadways: This feature appears to be quite distinct, especially for Type_1, which has a wider range and larger distances from roadways compared to Type_2 and Type_3.\n",
    "\n",
    "Horizontal Distance to Fire Points: This variable has some separation between vegetation types, with Type_1 having much higher distances to fire points than Type_2 and Type_3, which cluster lower on this axis.\n",
    "\n",
    "Canopy Density: All vegetation types appear to have similar canopy densities, making it difficult to differentiate between them based on this feature.\n",
    "\n",
    "Rainfall (Summer and Winter): The rainfall in both seasons seems to be very similar across vegetation types, showing little to no variation or overlap.\n",
    "\n",
    "Wind Exposure Level: There is minimal distinction among the vegetation types based on wind exposure, as all appear to have similar values.\n",
    "\n",
    "## Key Insights:\n",
    "Altitude and horizontal distance to roadways/fire points appear to be strong variables for separating vegetation types, particularly Type_1.\n",
    "Some features, like slope orientation, shadow indices, and canopy density, show a lot of overlap, suggesting they might not be as important in classification.\n",
    "Horizontal distance to water also provides some separability for Type_3, which might help in classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the style for the visualizations\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# List of numerical variables to plot against Vegetation_Type\n",
    "numerical_columns = [\n",
    "    'Altitude', 'Slope_Orientation', 'Slope', \n",
    "    'Horizontal_Distance_To_Water', 'Vertical_Distance_To_Water', \n",
    "    'Horizontal_Distance_To_Roadways', 'Shadow_Index_9h', \n",
    "    'Shadow_Index_12h', 'Shadow_Index_15h', \n",
    "    'Horizontal_Distance_To_Fire_Points', 'Canopy_Density', \n",
    "    'Rainfall_Summer', 'Rainfall_Winter', 'Wind_Exposure_Level'\n",
    "]\n",
    "\n",
    "# Set up the subplots grid\n",
    "num_plots = len(numerical_columns)\n",
    "cols = 3  # Number of columns\n",
    "rows = num_plots // cols + (num_plots % cols > 0)  # Number of rows\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(18, 4 * rows))\n",
    "fig.suptitle('Scatter Plots of Numerical Variables vs Vegetation Type', fontsize=16)\n",
    "\n",
    "# Plot each numerical variable vs Vegetation_Type\n",
    "for i, col in enumerate(numerical_columns):\n",
    "    row = i // cols\n",
    "    col_idx = i % cols\n",
    "    # Use stripplot for jitter effect or scatterplot directly\n",
    "    sns.stripplot(x='Vegetation_Type', y=col, data=df, ax=axes[row, col_idx], jitter=True, palette='Set2', hue='Vegetation_Type', alpha=0.6, legend=False)\n",
    "    axes[row, col_idx].set_title(f'{col} vs Vegetation Type')\n",
    "    axes[row, col_idx].tick_params(axis='x', rotation=45)  # Rotate x labels for readability\n",
    "\n",
    "# Hide any empty subplots (if any)\n",
    "for i in range(num_plots, rows * cols):\n",
    "    fig.delaxes(axes.flat[i])\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Observations from the Correlation Heatmap:\n",
    "Altitude: Strong negative correlation with Vegetation_Type_Encoded (-0.85), showing altitude plays a major role in distinguishing vegetation types.\n",
    "\n",
    "Horizontal_Distance_To_Roadways: Moderate negative correlation (-0.44), indicating distance from roadways helps differentiate vegetation types.\n",
    "\n",
    "Slope: Positive correlation (0.37), suggesting steeper slopes are more common in certain vegetation types.\n",
    "\n",
    "Horizontal_Distance_To_Fire_Points: Moderate negative correlation (-0.35), showing vegetation type is influenced by distance from fire points.\n",
    "\n",
    "Shadow Indices: Little correlation with Vegetation_Type_Encoded, implying minimal impact on vegetation classification.\n",
    "\n",
    "## Summary:\n",
    "Altitude, slope, and distances to roadways/fire points are key variables for distinguishing vegetation types.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Convert Vegetation_Type to numerical labels using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "df['Vegetation_Type_Encoded'] = label_encoder.fit_transform(df['Vegetation_Type'])\n",
    "\n",
    "# List of numerical columns to include in the correlation heatmap\n",
    "numerical_columns = [\n",
    "    'Altitude', 'Slope_Orientation', 'Slope', \n",
    "    'Horizontal_Distance_To_Water', 'Vertical_Distance_To_Water', \n",
    "    'Horizontal_Distance_To_Roadways', 'Shadow_Index_9h', \n",
    "    'Shadow_Index_12h', 'Shadow_Index_15h', \n",
    "    'Horizontal_Distance_To_Fire_Points', 'Canopy_Density', \n",
    "    'Rainfall_Summer', 'Rainfall_Winter', 'Wind_Exposure_Level',\n",
    "    'Vegetation_Type_Encoded'  # Include encoded Vegetation_Type\n",
    "]\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr_matrix = df[numerical_columns].corr()\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Correlation Heatmap of Numerical Variables')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Box plots for comparing categorical variables.\n",
    "\n",
    "In the graphs, we observe the relationship between Soil Type and Vegetation Type. The first two vegetation types (Type 1 and Type 2) show similar distributions for the various soil types. However, Vegetation Type 3 exhibits a distinct soil type distribution, indicating that it occurs in areas with different soil characteristics.\n",
    "\n",
    "For the Wilderness Area vs Vegetation Type plot, we see that Vegetation Types 1 and 2 share similar wilderness areas (Areas 1, 2, and 3). In contrast, Vegetation Type 3 appears to be predominantly associated with Area 4, suggesting a different wilderness distribution compared to the other vegetation types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the style for the visualizations\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Plot box plots for categorical variables vs Vegetation_Type\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "fig.suptitle('Box Plots of Categorical Variables vs Vegetation Type', fontsize=16)\n",
    "\n",
    "# Box plot for Soil_Type vs Vegetation_Type\n",
    "sns.boxplot(x='Vegetation_Type', y='Soil_Type', data=df, ax=axes[0], palette='Set2', hue='Vegetation_Type', legend=False)\n",
    "axes[0].set_title('Soil Type vs Vegetation Type')\n",
    "axes[0].set_xlabel('Vegetation Type')\n",
    "axes[0].set_ylabel('Soil Type')\n",
    "\n",
    "# Box plot for Wilderness_Area vs Vegetation_Type\n",
    "sns.boxplot(x='Vegetation_Type', y='Wilderness_Area', data=df, ax=axes[1], palette='Set2', hue='Vegetation_Type', legend=False)\n",
    "axes[1].set_title('Wilderness Area vs Vegetation Type')\n",
    "axes[1].set_xlabel('Vegetation Type')\n",
    "axes[1].set_ylabel('Wilderness Area')\n",
    "\n",
    "# Remove automatic legends\n",
    "axes[0].legend([], [], title='Vegetation Type', loc='upper right')\n",
    "axes[1].legend([], [], title='Vegetation Type', loc='upper right')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tryout LR all variables\n",
    "The model demonstrates good predictive ability with an accuracy of up to 84.6%. However, performance is weaker for Type_2 vegetation, which suggests room for improvement in capturing this class. Further efforts can be made to balance the model's performance across all vegetation types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 2: Load your dataset (adjust the file path as needed)\n",
    "# Assuming df is already loaded with the Vegetation_Type and Vegetation_Type_Encoded\n",
    "\n",
    "# Step 3: Prepare the data\n",
    "# Define numerical features\n",
    "numerical_features = [ \n",
    "    'Altitude', 'Slope_Orientation', 'Slope', \n",
    "    'Horizontal_Distance_To_Water', 'Vertical_Distance_To_Water', \n",
    "    'Horizontal_Distance_To_Roadways', 'Shadow_Index_9h', \n",
    "    'Shadow_Index_12h', 'Shadow_Index_15h', \n",
    "    'Horizontal_Distance_To_Fire_Points', 'Canopy_Density'\n",
    "    ,'Rainfall_Summer', 'Rainfall_Winter', 'Wind_Exposure_Level'\n",
    "]\n",
    "\n",
    "# Define categorical features\n",
    "categorical_features = ['Soil_Type', 'Wilderness_Area']  # Add your categorical columns here\n",
    "\n",
    "# Separate target variable (Vegetation_Type) and features\n",
    "X = df[numerical_features + categorical_features]  # Include both numerical and categorical\n",
    "y = df['Vegetation_Type']\n",
    "\n",
    "# Convert categorical variables into dummy variables\n",
    "X = pd.get_dummies(X, columns=categorical_features, drop_first=True)\n",
    "\n",
    "# Step 4: Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 5: Feature Scaling (Standardize the data)\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Step 6: Apply Logistic Regression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Step 7: Make predictions on the test data\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# Step 8: Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n",
    "\n",
    "# Step 9: Visualize the confusion matrix\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual Values')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is robust overall, achieving an accuracy of 84.6%. The main area for improvement lies in distinguishing Type_2, which shows lower recall compared to the other classes. I chosed only features with which ones I hadd best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 2: Load your dataset (adjust the file path as needed)\n",
    "# Assuming df is already loaded with the Vegetation_Type and Vegetation_Type_Encoded\n",
    "\n",
    "# Step 3: Prepare the data\n",
    "# Separate target variable (Vegetation_Type) and features\n",
    "y = df['Vegetation_Type']\n",
    "\n",
    "# Define numerical and categorical features\n",
    "numerical_features = ['Altitude', 'Slope_Orientation', 'Horizontal_Distance_To_Roadways',\n",
    "                      'Shadow_Index_9h', 'Shadow_Index_12h']\n",
    "\n",
    "categorical_features = ['Soil_Type', 'Wilderness_Area']  # Include your categorical columns\n",
    "\n",
    "# Convert categorical variables into dummy variables\n",
    "X_categorical = pd.get_dummies(df[categorical_features], drop_first=True)\n",
    "\n",
    "# Keep numerical variables as is\n",
    "X_numerical = df[numerical_features]\n",
    "\n",
    "# Combine numerical and dummy categorical variables\n",
    "X = pd.concat([X_numerical, X_categorical], axis=1)\n",
    "\n",
    "# Step 4: Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "# Step 5: Feature Scaling (Standardize the data)\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Step 6: Apply Logistic Regression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Step 7: Make predictions on the test data\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# Step 8: Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n",
    "\n",
    "# Step 9: Visualize the confusion matrix\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual Values')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA Tryout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y = df['Vegetation_Type']\n",
    "\n",
    "# Define numerical and categorical features\n",
    "numerical_features = ['Altitude', 'Slope_Orientation', 'Horizontal_Distance_To_Roadways',\n",
    "                      'Shadow_Index_9h', 'Shadow_Index_12h']\n",
    "\n",
    "categorical_features = ['Soil_Type', 'Wilderness_Area']  # Include your categorical columns\n",
    "\n",
    "# Step 3: Convert categorical variables into dummy variables\n",
    "# It's important to include only the selected features after converting to dummy\n",
    "X_categorical = pd.get_dummies(df[categorical_features], drop_first=True)\n",
    "\n",
    "# Keep numerical variables as is\n",
    "X_numerical = df[numerical_features]\n",
    "\n",
    "# Combine numerical and dummy categorical variables\n",
    "X = pd.concat([X_numerical, X_categorical], axis=1)\n",
    "\n",
    "# Step 4: Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 5: Feature Scaling (Standardize the data)\n",
    "scaler = MinMaxScaler()  # Create a StandardScaler object\n",
    "X_train = scaler.fit_transform(X_train)  # Fit and transform the training data\n",
    "X_test = scaler.transform(X_test)  # Only transform the test data\n",
    "\n",
    "# Step 6: Apply Linear Discriminant Analysis\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_train, y_train)\n",
    "\n",
    "# Step 7: Make predictions on the test data\n",
    "y_pred = lda.predict(X_test)\n",
    "\n",
    "# Step 8: Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n",
    "\n",
    "# Step 9: Visualize the confusion matrix\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual Values')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QDA TryOut, some warning tho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis  # Import QDA instead of LDA\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming df is already defined and loaded with your dataset\n",
    "y = df['Vegetation_Type']\n",
    "\n",
    "# Define numerical and categorical features\n",
    "numerical_features = ['Altitude', 'Slope_Orientation', 'Horizontal_Distance_To_Roadways',\n",
    "                      'Shadow_Index_9h', 'Shadow_Index_12h']\n",
    "\n",
    "categorical_features = ['Soil_Type', 'Wilderness_Area']  # Include your categorical columns\n",
    "\n",
    "# Step 3: Convert categorical variables into dummy variables\n",
    "# It's important to include only the selected features after converting to dummy\n",
    "X_categorical = pd.get_dummies(df[categorical_features], drop_first=True)\n",
    "\n",
    "# Keep numerical variables as is\n",
    "X_numerical = df[numerical_features]\n",
    "\n",
    "# Combine numerical and dummy categorical variables\n",
    "X = pd.concat([X_numerical, X_categorical], axis=1)\n",
    "\n",
    "# Step 4: Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 5: Feature Scaling (Standardize the data)\n",
    "scaler = MinMaxScaler()  # Create a MinMaxScaler object\n",
    "X_train = scaler.fit_transform(X_train)  # Fit and transform the training data\n",
    "X_test = scaler.transform(X_test)  # Only transform the test data\n",
    "\n",
    "# Step 6: Apply Quadratic Discriminant Analysis\n",
    "qda = QuadraticDiscriminantAnalysis()  # Create a QDA object\n",
    "qda.fit(X_train, y_train)  # Fit the QDA model to the training data\n",
    "\n",
    "# Step 7: Make predictions on the test data\n",
    "y_pred = qda.predict(X_test)\n",
    "\n",
    "# Step 8: Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n",
    "\n",
    "# Step 9: Visualize the confusion matrix\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual Values')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data for models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df, target, numerical_features, categorical_features):\n",
    "    # Combine numerical and categorical features\n",
    "    selected_features = numerical_features + categorical_features\n",
    "    \n",
    "    # Separate features and target variable\n",
    "    X = df[selected_features]\n",
    "    y = df[target]\n",
    "    \n",
    "    # Convert categorical variables into dummy variables\n",
    "    X = pd.get_dummies(X, drop_first=True)\n",
    "    \n",
    "    # Scale numerical features\n",
    "    scaler = MinMaxScaler()\n",
    "    X[numerical_features] = scaler.fit_transform(X[numerical_features])\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Holdout validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def holdout_validation(model, X, y, test_size=0.2):\n",
    "    # Train-Test Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "    \n",
    "    # Fit the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "    print(f\"Holdout Validation Accuracy: {accuracy}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "    print(\"Classification Report:\")\n",
    "    print(class_report)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    # sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "    # plt.title('Confusion Matrix (Holdout Validation)')\n",
    "    # plt.ylabel('Actual Values')\n",
    "    # plt.xlabel('Predicted Values')\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cross_validation(model, X, y, k=5):\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(model, X, y, cv=kf)\n",
    "    print(f\"K-Fold Cross Validation (k={k}) Accuracy Scores: {scores}\")\n",
    "    print(f\"Mean Accuracy: {np.mean(scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOOCV Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loocv_cross_validation(model, X, y, max_samples=None):\n",
    "    loo = LeaveOneOut()\n",
    "    \n",
    "    # Use max_samples to limit the number of LOOCV evaluations\n",
    "    if max_samples is not None and max_samples < len(X):\n",
    "        indices = np.random.choice(len(X), max_samples, replace=False)\n",
    "        X = X.iloc[indices]\n",
    "        y = y.iloc[indices]\n",
    "\n",
    "    accuracies = []\n",
    "    \n",
    "    for train_index, test_index in loo.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "    print(f\"LOOCV Accuracy: {np.mean(accuracies):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_evaluation(model, X, y, n_iterations=1000):\n",
    "    n_size = int(len(X) * 0.9)  # Use 90% of the data for bootstrap sample\n",
    "    scores = []\n",
    "\n",
    "    for i in range(n_iterations):\n",
    "        # Generate a bootstrap sample\n",
    "        X_train, y_train = resample(X, y, n_samples=n_size, random_state=i)\n",
    "        \n",
    "        # Fit the model on the bootstrap sample\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Test on the out-of-bag data (remaining 10%)\n",
    "        X_test, y_test = X.drop(X_train.index), y.drop(y_train.index)\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        scores.append(accuracy)\n",
    "\n",
    "    print(f\"Bootstrap Mean Accuracy: {np.mean(scores)}\")\n",
    "    print(f\"Bootstrap Standard Deviation: {np.std(scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline for running resempling in models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cross_validation_pipeline(df, model, target='Vegetation_Type', use_loocv=False, use_bootstrap=False):\n",
    "    # Define numerical and categorical features\n",
    "    numerical_features = ['Altitude', 'Slope_Orientation', 'Horizontal_Distance_To_Roadways', \n",
    "                          'Shadow_Index_9h', 'Shadow_Index_12h']\n",
    "    categorical_features = ['Soil_Type', 'Wilderness_Area']\n",
    "    \n",
    "    # Step 1: Preprocess the data\n",
    "    X, y = preprocess_data(df, target, numerical_features, categorical_features)\n",
    "\n",
    "    \n",
    "    # Step 2: Perform Holdout Validation\n",
    "    print(\"Holdout Validation Results:\")\n",
    "    holdout_validation(model, X, y, test_size=0.2)\n",
    "    \n",
    "    # Step 3: Perform K-Fold Cross Validation\n",
    "    print(\"\\nK-Fold Cross Validation Results (k=5):\")\n",
    "    k_fold_cross_validation(model, X, y, k=5)\n",
    "    \n",
    "    print(\"\\nK-Fold Cross Validation Results (k=10):\")\n",
    "    k_fold_cross_validation(model, X, y, k=10)\n",
    "    \n",
    "    # Step 4: Optionally Perform Leave-One-Out Cross Validation (LOOCV)\n",
    "    if use_loocv:\n",
    "        print(\"\\nLOOCV Results:\")\n",
    "        loocv_cross_validation(model, X, y, max_samples=1000)\n",
    "    \n",
    "    # Step 5: Optionally Perform Bootstrap Evaluation\n",
    "    if use_bootstrap:\n",
    "        print(\"\\nBootstrap Evaluation Results:\")\n",
    "        bootstrap_evaluation(model, X, y, n_iterations=1000)  # Limit Bootstrap iterations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying the types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_model = LogisticRegression()\n",
    "lda_model = LinearDiscriminantAnalysis()\n",
    "qda_model = QuadraticDiscriminantAnalysis()\n",
    "# Example usage without LOOCV and Bootstrap (default)\n",
    "#run_cross_validation_pipeline(df, logreg_model)\n",
    "\n",
    "# Example usage with LOOCV and Bootstrap (optional)\n",
    "run_cross_validation_pipeline(df, logreg_model, use_loocv=True, use_bootstrap=True)\n",
    "\n",
    "run_cross_validation_pipeline(df, lda_model, use_loocv=True, use_bootstrap=True)\n",
    "\n",
    "run_cross_validation_pipeline(df, qda_model, use_loocv=True, use_bootstrap=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion:\n",
    "Logistic Regression is the best-performing model based on both accuracy and F1 score, making it the preferred choice for this dataset.\n",
    "\n",
    "\n",
    "QDA underperforms significantly, indicating that its assumptions about feature distribution may not align well with the dataset. (It is maybe because of the warning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def compare_models(models, X_train, X_test, y_train, y_test):\n",
    "    comparison_results = []\n",
    "    \n",
    "    for name, model in models:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "        comparison_results.append({'Model': name, 'Accuracy': accuracy, 'F1 Score': f1})\n",
    "\n",
    "    return pd.DataFrame(comparison_results)\n",
    "\n",
    "# Example usage\n",
    "models = [('Logistic Regression', logreg_model), ('LDA', lda_model), ('QDA', qda_model)]\n",
    "compare_models(models, X_train, X_test, y_train, y_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
